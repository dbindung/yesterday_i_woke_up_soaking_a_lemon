{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data uploading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_y(data_json_file):\n",
    "    text_lst, query_lst, ans_lst, y = [], [], [], []\n",
    "    with open(data_json_file, 'r', encoding=\"utf-8\") as json_file:\n",
    "        json_list = list(json_file)\n",
    "        #print(json_list[0])\n",
    "        for json_str in json_list:\n",
    "            item = json.loads(json_str)\n",
    "            text = item['passage']['text']\n",
    "            #print(item['passage'].keys())\n",
    "            questions = item['passage']['questions']\n",
    "            for q in questions:\n",
    "                query = q['question']\n",
    "                ans = q['answers']\n",
    "                for a in ans:\n",
    "                    text_lst.append(text)\n",
    "                    query_lst.append(query)\n",
    "                    ans_lst.append(a['text'])\n",
    "                    y.append(a['label'])\n",
    "    return text_lst, query_lst, ans_lst, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_test(data_json_file):\n",
    "    X1 = []\n",
    "    X2 = []\n",
    "    X3 = []\n",
    "    with open(data_json_file, 'r', encoding=\"utf-8\") as json_file:\n",
    "        json_list = list(json_file)\n",
    "        for json_str in json_list:\n",
    "            item = json.loads(json_str)\n",
    "            text = item['passage']['text']\n",
    "            questions = item['passage']['questions']\n",
    "            for q in questions:\n",
    "                query = q['question']\n",
    "                ans = q['answers']\n",
    "                for a in ans:\n",
    "                    X1.append(text) #текст\n",
    "                    X2.append(query) #вопрос\n",
    "                    X3.append(a['text']) #ответ на вопрос\n",
    "    return X1, X2, X3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>query</th>\n",
       "      <th>ans</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(1) Но люди не могут существовать без природы,...</td>\n",
       "      <td>Где бегала шпана?</td>\n",
       "      <td>В парке.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(1) Но люди не могут существовать без природы,...</td>\n",
       "      <td>Где бегала шпана?</td>\n",
       "      <td>В лесу.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(1) Но люди не могут существовать без природы,...</td>\n",
       "      <td>Где бегала шпана?</td>\n",
       "      <td>Около подъезда.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(1) Но люди не могут существовать без природы,...</td>\n",
       "      <td>Почему Люда ударила Артема?</td>\n",
       "      <td>Он к ней приставал.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(1) Но люди не могут существовать без природы,...</td>\n",
       "      <td>Почему Люда ударила Артема?</td>\n",
       "      <td>Он ее оскорбил.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  (1) Но люди не могут существовать без природы,...   \n",
       "1  (1) Но люди не могут существовать без природы,...   \n",
       "2  (1) Но люди не могут существовать без природы,...   \n",
       "3  (1) Но люди не могут существовать без природы,...   \n",
       "4  (1) Но люди не могут существовать без природы,...   \n",
       "\n",
       "                         query                  ans  label  \n",
       "0            Где бегала шпана?             В парке.      1  \n",
       "1            Где бегала шпана?              В лесу.      0  \n",
       "2            Где бегала шпана?      Около подъезда.      0  \n",
       "3  Почему Люда ударила Артема?  Он к ней приставал.      1  \n",
       "4  Почему Люда ударила Артема?      Он ее оскорбил.      0  "
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text, query, ans, y_train = get_X_y('data/train.jsonl')\n",
    "\n",
    "train_df = pd.DataFrame({\n",
    "    'text': text,\n",
    "    'query': query,\n",
    "    'ans': ans,\n",
    "    'label': y_train\n",
    "})\n",
    "\n",
    "# в колонке 'text'\n",
    "# лежит сам текст, вопрос, ответ на этот вопрос\n",
    "# так как к одному тексту может быть несколько ответов, то похожих строк может быть несколько\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>query</th>\n",
       "      <th>ans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(1) Самый первый «остров» Архипелага возник в ...</td>\n",
       "      <td>Почему Солженицына перевозили спецконвоем?</td>\n",
       "      <td>Так перевозили особо важных заключенных.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(1) Самый первый «остров» Архипелага возник в ...</td>\n",
       "      <td>Почему Солженицына перевозили спецконвоем?</td>\n",
       "      <td>Потому, что был эмигрантом.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(1) Самый первый «остров» Архипелага возник в ...</td>\n",
       "      <td>Почему Солженицына перевозили спецконвоем?</td>\n",
       "      <td>Потому, что он сам вырыл себе землянку.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(1) Самый первый «остров» Архипелага возник в ...</td>\n",
       "      <td>Почему Солженицына перевозили спецконвоем?</td>\n",
       "      <td>Потому, что он побывал на пересылке Красная Пр...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(1) Самый первый «остров» Архипелага возник в ...</td>\n",
       "      <td>Почему Солженицына перевозили спецконвоем?</td>\n",
       "      <td>Потому, что он был особо важным заключённым и ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  (1) Самый первый «остров» Архипелага возник в ...   \n",
       "1  (1) Самый первый «остров» Архипелага возник в ...   \n",
       "2  (1) Самый первый «остров» Архипелага возник в ...   \n",
       "3  (1) Самый первый «остров» Архипелага возник в ...   \n",
       "4  (1) Самый первый «остров» Архипелага возник в ...   \n",
       "\n",
       "                                        query  \\\n",
       "0  Почему Солженицына перевозили спецконвоем?   \n",
       "1  Почему Солженицына перевозили спецконвоем?   \n",
       "2  Почему Солженицына перевозили спецконвоем?   \n",
       "3  Почему Солженицына перевозили спецконвоем?   \n",
       "4  Почему Солженицына перевозили спецконвоем?   \n",
       "\n",
       "                                                 ans  \n",
       "0           Так перевозили особо важных заключенных.  \n",
       "1                        Потому, что был эмигрантом.  \n",
       "2            Потому, что он сам вырыл себе землянку.  \n",
       "3  Потому, что он побывал на пересылке Красная Пр...  \n",
       "4  Потому, что он был особо важным заключённым и ...  "
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1, X2, X3 = get_X_test('data/val_wtihout.jsonl')\n",
    "\n",
    "test_df = pd.DataFrame({\n",
    "    'text': X1,\n",
    "    'query': X2,\n",
    "    'ans': X3\n",
    "})\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df[['text', 'query', 'ans']]\n",
    "y = train_df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "text_features = ['text', 'query', 'ans']\n",
    "\n",
    "model = CatBoostClassifier(\n",
    "    text_features=text_features,\n",
    "    use_best_model=False,\n",
    "    iterations=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, eval_set=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['label'] = model.predict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[['index', 'label']].to_csv('sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast, BertTokenizer, DistilBertModel, AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicCommentsDataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            data: pd.DataFrame,\n",
    "            tokenizer: BertTokenizer,\n",
    "            max_token_len: int=128,\n",
    "            test=False\n",
    "        ) -> None:\n",
    "\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_token_len = max_token_len\n",
    "        self.test = test\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> dict:\n",
    "        data_row = self.data.iloc[index]\n",
    "        \n",
    "        # Извлекаем текстовые поля\n",
    "        text = data_row['text']\n",
    "        query = data_row['query']\n",
    "        ans = data_row['ans']\n",
    "        \n",
    "        # Объединяем текст, query и ans в один вход с разделителями [SEP]\n",
    "        combined_text = f\"{text} [SEP] {query} [SEP] {ans}\"\n",
    "        \n",
    "        # Токенизация объединенного текста\n",
    "        encoding = self.tokenizer(\n",
    "            combined_text,\n",
    "            max_length=self.max_token_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Формируем словарь с одним набором input_ids и attention_mask\n",
    "        item = {\n",
    "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "        }\n",
    "        \n",
    "        # Если это тренировочный или валидационный набор, добавляем метку\n",
    "        if not self.test:\n",
    "            item[\"label\"] = torch.tensor(data_row['label'], dtype=torch.long)\n",
    "        \n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL_NAME = 'distilbert-base-uncased'\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(BERT_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=2008)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test=False\n",
    "train_dataset = ToxicCommentsDataset(\n",
    "  train_df,\n",
    "  tokenizer,\n",
    "  max_token_len=128\n",
    ")\n",
    "\n",
    "val_dataset = ToxicCommentsDataset(\n",
    "  val_df,\n",
    "  tokenizer,\n",
    "  max_token_len=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relatable_question(nn.Module):\n",
    "    def __init__(self, n_classes: int, n_training_steps=None):\n",
    "        super().__init__()\n",
    "        self.bert = DistilBertModel.from_pretrained(BERT_MODEL_NAME, return_dict=True)  # загрузка предобученной модели BERT\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes)  # добавление линейного слоя\n",
    "        self.n_training_steps = n_training_steps\n",
    "        \n",
    "        # for param in list(self.bert.parameters())[:-4]:\n",
    "        #     param.requires_grad = False\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = output.last_hidden_state[:, 0, :]\n",
    "        output = self.classifier(hidden_state)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Relatable_question(n_classes=2).to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, optimizer, loss_fn):\n",
    "    model = model.to(DEVICE)\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    \n",
    "    y_true = list()\n",
    "    y_pred = list()\n",
    "\n",
    "    for batch in tqdm(data_loader):\n",
    "        input_ids = batch['input_ids']\n",
    "        at_mask = batch['attention_mask']\n",
    "        y = batch['label']\n",
    "        input_ids, at_mask, y = input_ids.to(DEVICE), at_mask.to(DEVICE), y.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(input_ids, at_mask)\n",
    "\n",
    "        loss = loss_fn(output, y)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        total_loss += loss.item()\n",
    " \n",
    "        y_true.extend(y.tolist())\n",
    "        y_pred.extend(output.argmax(dim=1).tolist())\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    return total_loss / len(data_loader), accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, loss_fn):\n",
    "    model = model.to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    y_true = list()\n",
    "    y_pred = list()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader):\n",
    "            input_ids = batch['input_ids']\n",
    "            at_mask = batch['attention_mask']\n",
    "            y = batch['label']\n",
    "            input_ids, at_mask, y = input_ids.to(DEVICE), at_mask.to(DEVICE), y.to(DEVICE)\n",
    "            output = model(input_ids, at_mask)\n",
    "\n",
    "            loss = loss_fn(output, y)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            y_true.extend(y.tolist())\n",
    "            y_pred.extend(output.argmax(dim=1).tolist())\n",
    "\n",
    "    return total_loss / len(data_loader), accuracy_score(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stats(\n",
    "    train_loss: list[float],\n",
    "    valid_loss: list[float],\n",
    "    train_accuracy: list[float],\n",
    "    valid_accuracy: list[float],\n",
    "    title: str\n",
    "):\n",
    "    plt.figure(figsize=(16, 8))\n",
    "\n",
    "    plt.title(title + ' loss')\n",
    "\n",
    "    plt.plot(train_loss, label='Train loss')\n",
    "    plt.plot(valid_loss, label='Valid loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(16, 8))\n",
    "\n",
    "    plt.title(title + ' Accuracy')\n",
    "\n",
    "    plt.plot(train_accuracy, label='Train accuracy')\n",
    "    plt.plot(valid_accuracy, label='Valid accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, train_loader, valid_loader, optimizer, loss_fn, num_epochs, title='Model'):\n",
    "    train_loss_history, valid_loss_history = [], []\n",
    "    train_accuracy_history, valid_accuracy_history = [], []\n",
    "\n",
    "    best_valid_accuracy = 0.0\n",
    "    best_model = None\n",
    "\n",
    "    def epoch(count):\n",
    "        nonlocal best_valid_accuracy, best_model\n",
    "\n",
    "        train_loss, train_accuracy = train(model, train_loader, optimizer, loss_fn)\n",
    "        valid_loss, valid_accuracy = evaluate(model, valid_loader, loss_fn)\n",
    "\n",
    "        train_loss_history.append(train_loss)\n",
    "        valid_loss_history.append(valid_loss)\n",
    "\n",
    "        train_accuracy_history.append(train_accuracy)\n",
    "        valid_accuracy_history.append(valid_accuracy)\n",
    "\n",
    "        clear_output()\n",
    "\n",
    "        print(f\"Epoch: {count}\")\n",
    "        print(f\"Accuracy: {valid_accuracy:.4f}\")\n",
    "\n",
    "        if valid_accuracy > best_valid_accuracy:\n",
    "            best_valid_accuracy = valid_accuracy\n",
    "            best_model = deepcopy(model)\n",
    "\n",
    "    epoch(1)\n",
    "\n",
    "    for i in range(2, num_epochs + 1):\n",
    "        epoch(i)\n",
    "\n",
    "        plot_stats(\n",
    "            train_loss_history, valid_loss_history,\n",
    "            train_accuracy_history, valid_accuracy_history,\n",
    "            title\n",
    "        )\n",
    "\n",
    "    return best_model, best_valid_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Accuracy: 0.5569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/299 [00:00<01:30,  3.28it/s]"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "fit(model, train_dataloader, val_dataloader, optimizer, criterion, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30522, 768, padding_idx=0)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Теперь подстраиваем размер словаря модели под токенизатор\n",
    "model.bert.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер словаря токенизатора: 30522\n",
      "Размер словаря модели: 30522\n"
     ]
    }
   ],
   "source": [
    "print(\"Размер словаря токенизатора:\", len(tokenizer))\n",
    "print(\"Размер словаря модели:\", model.bert.config.vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ToxicCommentsDataset(\n",
    "  test_df,\n",
    "  tokenizer,\n",
    "  max_token_len=128,\n",
    "  test=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data_loader):\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader):\n",
    "            input_ids = batch['input_ids']\n",
    "            at_mask = batch['attention_mask']\n",
    "            input_ids, at_mask = input_ids.to(DEVICE), at_mask.to(DEVICE)\n",
    "            output = model(input_ids, at_mask)\n",
    "\n",
    "            y_pred.extend(output.argmax(dim=1).tolist())\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:10<00:00,  6.90it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(model, test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
