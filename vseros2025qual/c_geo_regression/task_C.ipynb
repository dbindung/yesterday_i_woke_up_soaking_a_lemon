{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YE6ux_PCigul"
      },
      "source": [
        "## Без catboost никуда"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ysc-hnuiXU9W"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import zipfile\n",
        "import os\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.neighbors import KDTree\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import warnings\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "import requests\n",
        "import geopandas as gpd\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "igSDLGYRTSoI"
      },
      "outputs": [],
      "source": [
        "def haversine(lat1, lon1, lat2=55.7558, lon2=37.6176):\n",
        "\n",
        "    R = 6371.0\n",
        "\n",
        "    lat1_rad = math.radians(lat1)\n",
        "    lon1_rad = math.radians(lon1)\n",
        "    lat2_rad = math.radians(lat2)\n",
        "    lon2_rad = math.radians(lon2)\n",
        "\n",
        "    dlat = lat2_rad - lat1_rad\n",
        "    dlon = lon2_rad - lon1_rad\n",
        "\n",
        "    a = math.sin(dlat / 2)**2 + math.cos(lat1_rad) * math.cos(lat2_rad) * math.sin(dlon / 2)**2\n",
        "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
        "\n",
        "    distance = R * c\n",
        "    return distance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWi4d2GiY3eY"
      },
      "source": [
        "## все выше - загрузка датасета и импорт библиотек!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7ShLqoNAX-gW"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv('data/train.tsv', sep='\\t')\n",
        "test_data = pd.read_csv('data/test.tsv', sep='\\t')\n",
        "reviews = pd.read_csv('data/reviews.txv/reviews.tsv', sep='\\t')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XoA727cG5PQa"
      },
      "outputs": [],
      "source": [
        "y_train = train_data['target']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 739
        },
        "id": "s5sPH7Akk8Sn",
        "outputId": "b43294a4-b5fd-4f0f-fd74-425f3a562c22"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>name</th>\n",
              "      <th>coordinates</th>\n",
              "      <th>category</th>\n",
              "      <th>address</th>\n",
              "      <th>target</th>\n",
              "      <th>traffic_300m</th>\n",
              "      <th>homes_300m</th>\n",
              "      <th>works_300m</th>\n",
              "      <th>female_300m</th>\n",
              "      <th>...</th>\n",
              "      <th>doramas_1000m</th>\n",
              "      <th>computer_components_1000m</th>\n",
              "      <th>humor_1000m</th>\n",
              "      <th>car_market_1000m</th>\n",
              "      <th>no_higher_education_1000m</th>\n",
              "      <th>goods_for_moms_and_babies_1000m</th>\n",
              "      <th>age_25-34_1000m</th>\n",
              "      <th>male_1000m</th>\n",
              "      <th>phone_repair_1000m</th>\n",
              "      <th>mean_income_1000m</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1365</td>\n",
              "      <td>Городская поликлиника № 109, филиал № 2</td>\n",
              "      <td>[37.735049, 55.719667]</td>\n",
              "      <td>health</td>\n",
              "      <td>Грайвороновская ул., 18, корп. 1, Москва</td>\n",
              "      <td>4.1</td>\n",
              "      <td>75429</td>\n",
              "      <td>16113.582471</td>\n",
              "      <td>15756.246444</td>\n",
              "      <td>51316.0</td>\n",
              "      <td>...</td>\n",
              "      <td>4668.0</td>\n",
              "      <td>7718.0</td>\n",
              "      <td>33389.0</td>\n",
              "      <td>18306.0</td>\n",
              "      <td>426241.0</td>\n",
              "      <td>415.0</td>\n",
              "      <td>380148.0</td>\n",
              "      <td>619550.0</td>\n",
              "      <td>2781.0</td>\n",
              "      <td>113767.387249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8230</td>\n",
              "      <td>Wellness Club Nebo</td>\n",
              "      <td>[37.537083, 55.749511]</td>\n",
              "      <td>swimming_pool</td>\n",
              "      <td>Пресненская наб., 12, Москва</td>\n",
              "      <td>3.6</td>\n",
              "      <td>246535</td>\n",
              "      <td>8578.458740</td>\n",
              "      <td>31315.672794</td>\n",
              "      <td>192547.0</td>\n",
              "      <td>...</td>\n",
              "      <td>3431.0</td>\n",
              "      <td>11463.0</td>\n",
              "      <td>61107.0</td>\n",
              "      <td>23662.0</td>\n",
              "      <td>488685.0</td>\n",
              "      <td>356.0</td>\n",
              "      <td>436721.0</td>\n",
              "      <td>764733.0</td>\n",
              "      <td>4264.0</td>\n",
              "      <td>122931.921255</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 286 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     id                                     name             coordinates  \\\n",
              "0  1365  Городская поликлиника № 109, филиал № 2  [37.735049, 55.719667]   \n",
              "1  8230                       Wellness Club Nebo  [37.537083, 55.749511]   \n",
              "\n",
              "        category                                   address  target  \\\n",
              "0         health  Грайвороновская ул., 18, корп. 1, Москва     4.1   \n",
              "1  swimming_pool              Пресненская наб., 12, Москва     3.6   \n",
              "\n",
              "   traffic_300m    homes_300m    works_300m  female_300m  ...  doramas_1000m  \\\n",
              "0         75429  16113.582471  15756.246444      51316.0  ...         4668.0   \n",
              "1        246535   8578.458740  31315.672794     192547.0  ...         3431.0   \n",
              "\n",
              "   computer_components_1000m  humor_1000m  car_market_1000m  \\\n",
              "0                     7718.0      33389.0           18306.0   \n",
              "1                    11463.0      61107.0           23662.0   \n",
              "\n",
              "   no_higher_education_1000m  goods_for_moms_and_babies_1000m  \\\n",
              "0                   426241.0                            415.0   \n",
              "1                   488685.0                            356.0   \n",
              "\n",
              "   age_25-34_1000m  male_1000m  phone_repair_1000m  mean_income_1000m  \n",
              "0         380148.0    619550.0              2781.0      113767.387249  \n",
              "1         436721.0    764733.0              4264.0      122931.921255  \n",
              "\n",
              "[2 rows x 286 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZmvBeR6pWMT",
        "outputId": "f7ab4623-e72c-46f8-9e8c-f7fff9ce3d98"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3986"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data[train_data['target'] < 1].shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "YAms88b3m3Kp",
        "outputId": "11a491c0-2d89-437e-91c9-d3c9fcceba38"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "address\n",
              "Кировоградская ул., 13А, Москва                           59\n",
              "Ходынский бул., 4, Москва                                 58\n",
              "просп. Мира, 211, корп. 2, Москва                         52\n",
              "площадь Киевского Вокзала, 2, Москва                      51\n",
              "Дмитровское ш., 163А, Москва                              49\n",
              "                                                          ..\n",
              "Армавирская ул., 1/20, Москва                              1\n",
              "Малый Толмачёвский пер., 8/11с3, Москва                    1\n",
              "Железнодорожный пр., с27, микрорайон Керамик, Балашиха     1\n",
              "Лётная ул., 27А, Мытищи                                    1\n",
              "ул. Панфёрова, 8, корп. 2, Москва                          1\n",
              "Name: count, Length: 22155, dtype: int64"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data['address'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgLKaKmxlZaA"
      },
      "source": [
        "# TF-IDF оказался полезнее BERT, прогнал еще через SVD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Qno4fGp-W7dn"
      },
      "outputs": [],
      "source": [
        "\n",
        "def extract_text_features(reviews_df):\n",
        "\n",
        "\n",
        "    reviews_agg = reviews_df.groupby('id')['text'].agg(' '.join).reset_index()\n",
        "\n",
        "\n",
        "    tfidf = TfidfVectorizer(\n",
        "        max_features=5000,\n",
        "        ngram_range=(1, 3),\n",
        "        stop_words=['и', 'в', 'на', 'с', 'у', 'к', 'по', 'для', 'это', 'то', 'так']\n",
        "    )\n",
        "    tfidf_matrix = tfidf.fit_transform(reviews_agg['text'])\n",
        "\n",
        "\n",
        "    svd = TruncatedSVD(n_components=100, random_state=42)\n",
        "    tfidf_reduced = svd.fit_transform(tfidf_matrix)\n",
        "\n",
        "\n",
        "\n",
        "    return reviews_agg, tfidf_reduced,\n",
        "reviews_agg, tfidf_reduced =extract_text_features(reviews)\n",
        "import pandas as pd\n",
        "\n",
        "tfidf_array = tfidf_reduced\n",
        "tfidf_df = pd.DataFrame(tfidf_array, index=reviews_agg.index)\n",
        "\n",
        "tfidf_df.columns = [f'tfidf_{i}' for i in range(tfidf_df.shape[1])]\n",
        "\n",
        "result = pd.concat([reviews_agg, tfidf_df], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "tabDLz9eQk9m",
        "outputId": "a0eb8529-1d9a-4abe-fc41-277425f60e2d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n\\ndef extract_bert_features(reviews_df, model_name=\"deeppavlov/rubert-base-cased\"):\\n\\n\\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\n\\n    reviews_agg = reviews_df.groupby(\"id\")[\"text\"].agg(\" \".join).reset_index()\\n\\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\\n    model = AutoModel.from_pretrained(model_name).to(device)\\n    model.eval()\\n\\n    embeddings = []\\n\\n    for text in tqdm(reviews_agg[\"text\"], desc=\"Extracting BERT embeddings\"):\\n        inputs = tokenizer(\\n            text,\\n            return_tensors=\"pt\",\\n            truncation=True,\\n            max_length=128,\\n            padding=\"max_length\"\\n        ).to(device)\\n\\n        with torch.no_grad():\\n            outputs = model(**inputs)\\n            emb = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\\n\\n        embeddings.append(emb[0])\\n\\n    bert_array = np.vstack(embeddings)\\n    bert_df = pd.DataFrame(bert_array, columns=[f\"bert_{i}\" for i in range(bert_array.shape[1])])\\n\\n    result = pd.concat([reviews_agg, bert_df], axis=1)\\n    return reviews_agg, bert_df, result'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "\n",
        "def extract_bert_features(reviews_df, model_name=\"deeppavlov/rubert-base-cased\"):\n",
        "\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    reviews_agg = reviews_df.groupby(\"id\")[\"text\"].agg(\" \".join).reset_index()\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModel.from_pretrained(model_name).to(device)\n",
        "    model.eval()\n",
        "\n",
        "    embeddings = []\n",
        "\n",
        "    for text in tqdm(reviews_agg[\"text\"], desc=\"Extracting BERT embeddings\"):\n",
        "        inputs = tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=128,\n",
        "            padding=\"max_length\"\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            emb = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
        "\n",
        "        embeddings.append(emb[0])\n",
        "\n",
        "    bert_array = np.vstack(embeddings)\n",
        "    bert_df = pd.DataFrame(bert_array, columns=[f\"bert_{i}\" for i in range(bert_array.shape[1])])\n",
        "\n",
        "    result = pd.concat([reviews_agg, bert_df], axis=1)\n",
        "    return reviews_agg, bert_df, result'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "S6wmCiXVQp6_",
        "outputId": "92da3a2c-d2ac-416e-87ff-ee95ad579f1d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"reviews_agg_bert, bert_df, result_bert = extract_bert_features(reviews)\\n\\nfull_features = result.merge(result_bert.drop(columns=['text']), on='id', how='left')\""
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''reviews_agg_bert, bert_df, result_bert = extract_bert_features(reviews)\n",
        "\n",
        "full_features = result.merge(result_bert.drop(columns=['text']), on='id', how='left')'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "lrhq-S_XC6Ff"
      },
      "outputs": [],
      "source": [
        "reviews_agg, tfidf_reduced = extract_text_features(reviews)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "bMPrbBafC95g"
      },
      "outputs": [],
      "source": [
        "\n",
        "tfidf_array = tfidf_reduced\n",
        "tfidf_df = pd.DataFrame(tfidf_array, index=reviews_agg.index)\n",
        "\n",
        "tfidf_df.columns = [f'tfidf_{i}' for i in range(tfidf_df.shape[1])]\n",
        "\n",
        "result = pd.concat([reviews_agg, tfidf_df], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "8srVbgQGkqDD",
        "outputId": "66f49dcf-044b-45a5-8459-7eae436d11c1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\nassert 'id' in result.columns and 'id' in result_bert.columns\\n\\ntext_features_full = (\\n    result\\n    .drop(columns=['text'], errors='ignore')\\n    .merge(\\n        result_bert.drop(columns=['text'], errors='ignore'),\\n        on='id',\\n        how='outer'\\n    )\\n)\\n\""
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "assert 'id' in result.columns and 'id' in result_bert.columns\n",
        "\n",
        "text_features_full = (\n",
        "    result\n",
        "    .drop(columns=['text'], errors='ignore')\n",
        "    .merge(\n",
        "        result_bert.drop(columns=['text'], errors='ignore'),\n",
        "        on='id',\n",
        "        how='outer'\n",
        "    )\n",
        ")\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpfBfoneQizy"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydJHM_tJlljG"
      },
      "source": [
        "# Работает с основной частью данных, развертываем coordinates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "gYiZo5QPfzU-"
      },
      "outputs": [],
      "source": [
        "train_data = train_data[train_data['target'] >= 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "T-QEAF6UA4EM"
      },
      "outputs": [],
      "source": [
        "train_data[['longitude', 'latitude']] = train_data['coordinates'].str.strip('[]').str.split(',', expand=True).astype(float)\n",
        "test_data[['longitude', 'latitude']] = test_data['coordinates'].str.strip('[]').str.split(',', expand=True).astype(float)\n",
        "\n",
        "train_data['distance_to_moscow_center_km'] = train_data.apply(\n",
        "    lambda row: haversine(row['latitude'], row['longitude']), axis=1\n",
        ")\n",
        "test_data['distance_to_moscow_center_km'] = test_data.apply(\n",
        "    lambda row: haversine(row['latitude'], row['longitude']), axis=1\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF09bYUjlt1C"
      },
      "source": [
        "# Берем абсолютно все из текста, мне это в итоге помогло"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "LrJX8JRsnj72"
      },
      "outputs": [],
      "source": [
        "\n",
        "def add_review_features(main_data, reviews_file='/content/reviews.txv/reviews.tsv'):\n",
        "\n",
        "    try:\n",
        "        reviews = pd.read_csv(reviews_file, sep='\\t')\n",
        "\n",
        "        reviews['text_length'] = reviews['text'].str.len().fillna(0)\n",
        "        reviews['word_count'] = reviews['text'].str.split().str.len().fillna(0)\n",
        "        reviews['char_count'] = reviews['text'].str.replace(' ', '').str.len().fillna(0)\n",
        "\n",
        "        reviews['exclamation_count'] = reviews['text'].str.count('!').fillna(0)\n",
        "        reviews['question_count'] = reviews['text'].str.count('\\?').fillna(0)\n",
        "        reviews['capital_ratio'] = reviews['text'].apply(\n",
        "            lambda x: sum(1 for char in str(x) if char.isupper()) / len(str(x)) if len(str(x)) > 0 else 0\n",
        "        )\n",
        "\n",
        "        reviews['comma_count'] = reviews['text'].str.count(',').fillna(0)\n",
        "        reviews['dot_count'] = reviews['text'].str.count('\\.').fillna(0)\n",
        "        reviews['ellipsis_count'] = reviews['text'].str.count('\\.\\.\\.').fillna(0)\n",
        "        reviews['line_breaks_count'] = reviews['text'].str.count('\\n').fillna(0)\n",
        "        reviews['has_quotes'] = reviews['text'].str.contains('\"|\\'').fillna(0).astype(int)\n",
        "\n",
        "        def simple_sentiment_analysis(text):\n",
        "            if pd.isna(text):\n",
        "                return 0\n",
        "            text = str(text).lower()\n",
        "            positive_words = ['хорош', 'отличн', 'прекрасн', 'рекоменд', 'супер',\n",
        "                            'класс', 'любим', 'удовольств', 'замечательн', 'восхитительн',\n",
        "                            'быстро', 'вежлив', 'чист', 'комфортн', 'вкусн', 'спасибо',\n",
        "                            'благодар', 'совету', 'порадова', 'понрави']\n",
        "            negative_words = ['плох', 'ужасн', 'кошмар', 'разочарован', 'не рекоменд',\n",
        "                            'отвратительн', 'груб', 'грязн', 'дорог', 'долго',\n",
        "                            'медленно', 'невкусн', 'шумн', 'тесн', 'обман', 'жаль',\n",
        "                            'напугал', 'отврат', 'противн', 'увол']\n",
        "\n",
        "            pos_count = sum(1 for word in positive_words if word in text)\n",
        "            neg_count = sum(1 for word in negative_words if word in text)\n",
        "\n",
        "            total = pos_count + neg_count\n",
        "            if total > 0:\n",
        "                return (pos_count - neg_count) / total\n",
        "            return 0\n",
        "\n",
        "        reviews['sentiment'] = reviews['text'].apply(simple_sentiment_analysis)\n",
        "\n",
        "\n",
        "        def get_sentiment_categories(text):\n",
        "            if pd.isna(text):\n",
        "                return 0, 0, 0\n",
        "            text = str(text).lower()\n",
        "\n",
        "\n",
        "            service_words = ['обслуживан', 'персонал', 'сотрудник', 'администратор', 'официант']\n",
        "            quality_words = ['качеств', 'уровен', 'стандарт', 'профессионал']\n",
        "            price_words = ['цен', 'стоим', 'дорог', 'дешев', 'выгодн']\n",
        "            atmosphere_words = ['атмосфер', 'интерьер', 'уютн', 'комфорт', 'обстановк']\n",
        "\n",
        "            service_count = sum(1 for word in service_words if word in text)\n",
        "            quality_count = sum(1 for word in quality_words if word in text)\n",
        "            price_count = sum(1 for word in price_words if word in text)\n",
        "            atmosphere_count = sum(1 for word in atmosphere_words if word in text)\n",
        "\n",
        "            total_mentions = service_count + quality_count + price_count + atmosphere_count\n",
        "            if total_mentions > 0:\n",
        "                return service_count/total_mentions, quality_count/total_mentions, price_count/total_mentions\n",
        "            return 0, 0, 0\n",
        "\n",
        "        reviews[['service_mention_ratio', 'quality_mention_ratio', 'price_mention_ratio']] = \\\n",
        "            reviews['text'].apply(lambda x: pd.Series(get_sentiment_categories(x)))\n",
        "\n",
        "        reviews['unique_words_ratio'] = reviews['text'].apply(\n",
        "            lambda x: len(set(str(x).split())) / len(str(x).split()) if len(str(x).split()) > 0 else 0\n",
        "        )\n",
        "\n",
        "        reviews['avg_word_length'] = reviews['char_count'] / reviews['word_count'].replace(0, 1)\n",
        "        reviews['has_emoji'] = reviews['text'].str.contains('[^\\x00-\\x7F]').fillna(0).astype(int)\n",
        "\n",
        "        reviews['long_words_ratio'] = reviews['text'].apply(\n",
        "            lambda x: sum(1 for word in str(x).split() if len(word) > 6) / len(str(x).split())\n",
        "            if len(str(x).split()) > 0 else 0\n",
        "        )\n",
        "\n",
        "        reviews['sentence_count'] = reviews['text'].str.count('[.!?]+').fillna(0)\n",
        "        reviews['avg_sentence_length'] = reviews['word_count'] / reviews['sentence_count'].replace(0, 1)\n",
        "\n",
        "        review_stats = reviews.groupby('id').agg({\n",
        "            'text': 'count',\n",
        "            'text_length': ['mean', 'std', 'max', 'min'],\n",
        "            'word_count': ['mean', 'std', 'max', 'min'],\n",
        "            'char_count': ['mean', 'std'],\n",
        "            'exclamation_count': ['mean', 'sum', 'max'],\n",
        "            'question_count': ['mean', 'sum', 'max'],\n",
        "            'capital_ratio': 'mean',\n",
        "            'comma_count': 'mean',\n",
        "            'dot_count': 'mean',\n",
        "            'ellipsis_count': 'sum',\n",
        "            'line_breaks_count': 'mean',\n",
        "            'has_quotes': 'mean',\n",
        "            'sentiment': ['mean', 'std', 'min', 'max', lambda x: (x > 0.1).sum(), lambda x: (x < -0.1).sum()],\n",
        "            'service_mention_ratio': 'mean',\n",
        "            'quality_mention_ratio': 'mean',\n",
        "            'price_mention_ratio': 'mean',\n",
        "            'unique_words_ratio': 'mean',\n",
        "            'avg_word_length': 'mean',\n",
        "            'has_emoji': 'mean',\n",
        "            'long_words_ratio': 'mean',\n",
        "            'sentence_count': 'mean',\n",
        "            'avg_sentence_length': 'mean'\n",
        "        }).reset_index()\n",
        "\n",
        "        review_stats.columns = [\n",
        "            'id',\n",
        "            'review_count',\n",
        "            'avg_text_length', 'std_text_length', 'max_text_length', 'min_text_length',\n",
        "            'avg_word_count', 'std_word_count', 'max_word_count', 'min_word_count',\n",
        "            'avg_char_count', 'std_char_count',\n",
        "            'avg_exclamation', 'total_exclamation', 'max_exclamation',\n",
        "            'avg_question', 'total_question', 'max_question',\n",
        "            'avg_capital_ratio',\n",
        "            'avg_comma_count',\n",
        "            'avg_dot_count',\n",
        "            'total_ellipsis',\n",
        "            'avg_line_breaks',\n",
        "            'quotes_ratio',\n",
        "            'avg_sentiment', 'std_sentiment', 'min_sentiment', 'max_sentiment', 'positive_reviews_count', 'negative_reviews_count',\n",
        "            'avg_service_mention',\n",
        "            'avg_quality_mention',\n",
        "            'avg_price_mention',\n",
        "            'avg_unique_words_ratio',\n",
        "            'avg_word_length',\n",
        "            'emoji_ratio',\n",
        "            'avg_long_words_ratio',\n",
        "            'avg_sentence_count',\n",
        "            'avg_sentence_length'\n",
        "        ]\n",
        "        review_stats['sentiment_volatility'] = review_stats['std_sentiment'].fillna(0)\n",
        "        review_stats['text_length_variation'] = review_stats['std_text_length'] / review_stats['avg_text_length'].replace(0, 1)\n",
        "        review_stats['text_length_variation'] = review_stats['text_length_variation'].replace([np.inf, -np.inf], 0)\n",
        "        review_stats['emotional_intensity'] = (review_stats['avg_exclamation'] + review_stats['avg_question']) / 2\n",
        "        review_stats['review_engagement'] = (review_stats['avg_text_length'] * review_stats['emotional_intensity'])\n",
        "        review_stats['has_positive_reviews'] = (review_stats['max_sentiment'] > 0.1).astype(int)\n",
        "        review_stats['has_negative_reviews'] = (review_stats['min_sentiment'] < -0.1).astype(int)\n",
        "        review_stats['has_mixed_reviews'] = ((review_stats['max_sentiment'] > 0.1) &\n",
        "                                           (review_stats['min_sentiment'] < -0.1)).astype(int)\n",
        "\n",
        "        review_stats['positive_negative_ratio'] = review_stats['positive_reviews_count'] / (review_stats['negative_reviews_count'] + 1)\n",
        "        review_stats['sentiment_balance'] = (review_stats['positive_reviews_count'] - review_stats['negative_reviews_count']) / review_stats['review_count'].replace(0, 1)\n",
        "        review_stats['text_complexity_index'] = (review_stats['avg_long_words_ratio'] + review_stats['avg_unique_words_ratio']) / 2\n",
        "        review_stats['punctuation_diversity'] = (review_stats['avg_exclamation'] + review_stats['avg_question'] + review_stats['avg_comma_count']) / 3\n",
        "        review_stats['has_detailed_reviews'] = (review_stats['avg_word_count'] > 20).astype(int)\n",
        "        review_stats['has_emotional_reviews'] = (review_stats['emotional_intensity'] > 0.5).astype(int)\n",
        "        review_stats['has_structured_reviews'] = (review_stats['avg_sentence_count'] > 2).astype(int)\n",
        "\n",
        "        main_data = main_data.merge(review_stats, on='id', how='left')\n",
        "\n",
        "        review_columns = [col for col in review_stats.columns if col != 'id']\n",
        "        for col in review_columns:\n",
        "            if 'count' in col or 'total' in col or 'max' in col or 'min' in col:\n",
        "                main_data[col] = main_data[col].fillna(0)\n",
        "            elif col.startswith('has_') or col.endswith('_ratio') or 'balance' in col or 'index' in col:\n",
        "                main_data[col] = main_data[col].fillna(0)\n",
        "            else:\n",
        "                main_data[col] = main_data[col].fillna(0)\n",
        "\n",
        "\n",
        "\n",
        "        return main_data\n",
        "\n",
        "    except Exception as e:\n",
        "\n",
        "\n",
        "        base_features = [\n",
        "            'review_count', 'avg_text_length', 'std_text_length', 'max_text_length', 'min_text_length',\n",
        "            'avg_word_count', 'std_word_count', 'max_word_count', 'min_word_count', 'avg_char_count',\n",
        "            'std_char_count', 'avg_exclamation', 'total_exclamation', 'max_exclamation', 'avg_question',\n",
        "            'total_question', 'max_question', 'avg_capital_ratio', 'avg_comma_count', 'avg_dot_count',\n",
        "            'total_ellipsis', 'avg_line_breaks', 'quotes_ratio', 'avg_sentiment', 'std_sentiment',\n",
        "            'min_sentiment', 'max_sentiment', 'positive_reviews_count', 'negative_reviews_count',\n",
        "            'avg_service_mention', 'avg_quality_mention', 'avg_price_mention', 'avg_unique_words_ratio',\n",
        "            'avg_word_length', 'emoji_ratio', 'avg_long_words_ratio', 'avg_sentence_count',\n",
        "            'avg_sentence_length', 'sentiment_volatility', 'text_length_variation', 'emotional_intensity',\n",
        "            'review_engagement', 'has_positive_reviews', 'has_negative_reviews', 'has_mixed_reviews',\n",
        "            'positive_negative_ratio', 'sentiment_balance', 'text_complexity_index', 'punctuation_diversity',\n",
        "            'has_detailed_reviews', 'has_emotional_reviews', 'has_structured_reviews'\n",
        "        ]\n",
        "\n",
        "        for feature in base_features:\n",
        "            main_data[feature] = 0\n",
        "\n",
        "        return main_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "h2ek4xKf8iYe"
      },
      "outputs": [],
      "source": [
        "train_data = add_review_features(train_data)\n",
        "test_data = add_review_features(test_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BPpcx65l04e"
      },
      "source": [
        "# Хочется ввести балльную систему по доходам"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "zp-hojoP_Nck"
      },
      "outputs": [],
      "source": [
        "def fqs(df):\n",
        "  income_weights = {\n",
        "      'below_average_income_1000m': 1,\n",
        "      'average_income_1000m': 2,\n",
        "      'above_average_income_1000m': 3,\n",
        "      'high_income_1000m': 4,\n",
        "      'premium_income_1000m': 5\n",
        "  }\n",
        "\n",
        "  df['wealth_numerator_1000m'] = sum(\n",
        "      df[col] * weight for col, weight in income_weights.items()\n",
        "  )\n",
        "\n",
        "  df['wealth_denominator_1000m'] = df[\n",
        "      list(income_weights.keys())\n",
        "  ].sum(axis=1) + 1e-6\n",
        "\n",
        "\n",
        "  df['wealth_index_1000m'] = df['wealth_numerator_1000m'] / df['wealth_denominator_1000m']\n",
        "\n",
        "  df.drop(['wealth_numerator_1000m', 'wealth_denominator_1000m'], axis=1, inplace=True)\n",
        "  return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "zKV7gDn7_VzA"
      },
      "outputs": [],
      "source": [
        "train_data = fqs(train_data)\n",
        "test_data = fqs(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "nsi7rEmgd7cd"
      },
      "outputs": [],
      "source": [
        "category_freq = pd.concat([train_data['category'], test_data['category']], axis=0).value_counts()\n",
        "train_data['category_freq'] = train_data['category'].map(category_freq)\n",
        "test_data['category_freq'] = test_data['category'].map(category_freq).fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "GTaaIvFnek2V"
      },
      "outputs": [],
      "source": [
        "category_target_mean = train_data.groupby('category')['target'].mean()\n",
        "train_data['category_target_enc'] = train_data['category'].map(category_target_mean)\n",
        "test_data['category_target_enc'] = test_data['category'].map(category_target_mean).fillna(train_data['target'].mean())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "s1e-ZEeqEai6"
      },
      "outputs": [],
      "source": [
        "coords_all = pd.concat([\n",
        "    train_data[[\"longitude\", \"latitude\"]],\n",
        "    test_data[[\"longitude\", \"latitude\"]]\n",
        "], axis=0, ignore_index=True)\n",
        "\n",
        "\n",
        "N_CLUSTERS = 100\n",
        "kmeans = KMeans(n_clusters=N_CLUSTERS, random_state=42, n_init=\"auto\")\n",
        "\n",
        "\n",
        "coords_all[\"cluster\"] = kmeans.fit_predict(coords_all)\n",
        "\n",
        "train_data[\"cluster\"] = coords_all[\"cluster\"].iloc[:len(train_data)].values\n",
        "test_data[\"cluster\"] = coords_all[\"cluster\"].iloc[len(train_data):].values\n",
        "\n",
        "\n",
        "cluster_stats = (\n",
        "    train_data.groupby(\"cluster\")[\"target\"]\n",
        "    .agg([\n",
        "        (\"mean_target_in_cluster\", \"mean\"),\n",
        "        (\"std_target_in_cluster\", \"std\"),\n",
        "        (\"count_in_cluster\", \"count\"),\n",
        "        (\"median_target_in_cluster\", \"median\"),\n",
        "    ])\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "train_data = train_data.merge(cluster_stats, on=\"cluster\", how=\"left\")\n",
        "test_data = test_data.merge(cluster_stats, on=\"cluster\", how=\"left\")\n",
        "\n",
        "train_data[\"std_target_in_cluster\"].fillna(0, inplace=True)\n",
        "test_data[\"std_target_in_cluster\"].fillna(0, inplace=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Oo9btVyfGuj2"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import BallTree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "D1aRFq_1GdId"
      },
      "outputs": [],
      "source": [
        "\n",
        "def add_density_features(df):\n",
        "\n",
        "    coords = np.radians(df[['latitude', 'longitude']].values)\n",
        "\n",
        "    tree = BallTree(coords, metric='haversine')\n",
        "\n",
        "    r_300m = 0.3 / 6371\n",
        "    r_1000m = 1.0 / 6371\n",
        "\n",
        "    count_300m = tree.query_radius(coords, r=r_300m, count_only=True)\n",
        "    count_1000m = tree.query_radius(coords, r=r_1000m, count_only=True)\n",
        "\n",
        "    df['density_300m'] = count_300m - 1\n",
        "    df['density_1000m'] = count_1000m - 1\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "w8emEk_WLTz2"
      },
      "outputs": [],
      "source": [
        "def add_same_category_features(df):\n",
        "\n",
        "    R = 6371\n",
        "    r_300m = 0.3 / R\n",
        "    r_1000m = 1.0 / R\n",
        "\n",
        "    df['latitude_rad'] = np.radians(df['latitude'])\n",
        "    df['longitude_rad'] = np.radians(df['longitude'])\n",
        "\n",
        "\n",
        "    df['nearest_same_category_dist'] = np.nan\n",
        "    df['same_category_count_300m'] = 0\n",
        "    df['same_category_count_1000m'] = 0\n",
        "\n",
        "    for cat in df['category'].unique():\n",
        "        mask = df['category'] == cat\n",
        "        coords = np.vstack((df.loc[mask, 'latitude_rad'], df.loc[mask, 'longitude_rad'])).T\n",
        "\n",
        "        if len(coords) < 2:\n",
        "            df.loc[mask, ['nearest_same_category_dist',\n",
        "                          'same_category_count_300m',\n",
        "                          'same_category_count_1000m']] = 0\n",
        "            continue\n",
        "\n",
        "\n",
        "        tree = BallTree(coords, metric='haversine')\n",
        "\n",
        "\n",
        "        dist, _ = tree.query(coords, k=2)\n",
        "        nearest_dist = dist[:, 1] * R * 1000\n",
        "\n",
        "\n",
        "        count_300m = tree.query_radius(coords, r=r_300m, count_only=True) - 1\n",
        "        count_1000m = tree.query_radius(coords, r=r_1000m, count_only=True) - 1\n",
        "\n",
        "        df.loc[mask, 'nearest_same_category_dist'] = nearest_dist\n",
        "        df.loc[mask, 'same_category_count_300m'] = count_300m\n",
        "        df.loc[mask, 'same_category_count_1000m'] = count_1000m\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "NSMXAhBCGRMk"
      },
      "outputs": [],
      "source": [
        "concatef = pd.concat([train_data.drop(columns=['target']), test_data], axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "0d37bLh6GeKR"
      },
      "outputs": [],
      "source": [
        "concatef = add_density_features(concatef)\n",
        "concatef = add_same_category_features(concatef)\n",
        "\n",
        "\n",
        "address_counts = concatef['address'].value_counts()\n",
        "name_counts = concatef['name'].value_counts()\n",
        "category_counts = concatef['category'].value_counts()\n",
        "\n",
        "concatef['address_count'] = concatef['address'].map(address_counts)\n",
        "concatef['name_count'] = concatef['name'].map(name_counts)\n",
        "concatef['category_count'] = concatef['category'].map(category_counts)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "qxZXj2_8GxzW"
      },
      "outputs": [],
      "source": [
        "train_data = pd.concat([concatef[:train_data.shape[0]], train_data['target']], axis=1)\n",
        "test_data = concatef[train_data.shape[0]:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "a_i5xmDPzHU-",
        "outputId": "a892f2a9-4880-42c8-b664-e71585749e09"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "address          20\n",
              "address_count    20\n",
              "dtype: int64"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data.isna().sum()[train_data.isna().sum() > 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "UUGKlQxCGN-R"
      },
      "outputs": [],
      "source": [
        "url = \"https://api.hh.ru/metro/1\"\n",
        "response = requests.get(url)\n",
        "if response.status_code == 200:\n",
        "    metro_data = response.json()\n",
        "metros = []\n",
        "for line in range(len(metro_data['lines'])):\n",
        "    here = metro_data['lines'][line]['stations']\n",
        "    for station in range(len(here)):\n",
        "        metros.append([here[station]['name'], here[station]['lng'], here[station]['lat']])\n",
        "\n",
        "metros = pd.DataFrame(metros, columns=['metro_name', 'lng', 'lat'])\n",
        "geo_metros = gpd.GeoDataFrame(\n",
        "    metros,\n",
        "    geometry=gpd.points_from_xy(metros.lng, metros.lat),\n",
        "    crs=\"EPSG:4326\"\n",
        ")\n",
        "geo_metros = geo_metros.drop_duplicates(subset=['metro_name']).reset_index(drop=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "16JbFAJ1LMsp"
      },
      "outputs": [],
      "source": [
        "def add_nearest_metros(df, geo_metros, n_neighbors=3):\n",
        "    df = df.copy()\n",
        "    df[['lng', 'lat']] = df['coordinates'].apply(lambda x: pd.Series(eval(x)))\n",
        "    geo_df = gpd.GeoDataFrame(\n",
        "        df,\n",
        "        geometry=gpd.points_from_xy(df.lng, df.lat),\n",
        "        crs=\"EPSG:4326\"\n",
        "    )\n",
        "    metros_radians = np.deg2rad(geo_metros[['lat', 'lng']])\n",
        "    places_radians = np.deg2rad(geo_df[['lat', 'lng']])\n",
        "\n",
        "    tree = BallTree(metros_radians, metric='haversine')\n",
        "    dist, ind = tree.query(places_radians, k=n_neighbors)\n",
        "    dist_km = dist * 6371\n",
        "    for i in range(n_neighbors):\n",
        "        geo_df[f'metro_{i+1}_name'] = geo_metros.iloc[ind[:, i]]['metro_name'].values\n",
        "        geo_df[f'metro_{i+1}_dist_km'] = dist_km[:, i]\n",
        "    geo_df = geo_df.drop(columns='geometry')\n",
        "    return geo_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# auto ml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "from autofeat import AutoFeatRegressor\n",
        "\n",
        "# 1. Берем числовые колонки\n",
        "num_cols = train_data.select_dtypes(include=['int', 'float']).columns.tolist()\n",
        "# исключаем target и, если хочешь, геокоординаты в радианах\n",
        "num_cols = [c for c in num_cols if c not in ['target', 'latitude_rad', 'longitude_rad', 'id', 'address_count', 'name_count', 'category_count']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-12 22:31:11,833 INFO: [AutoFeat] The 2 step feature engineering process could generate up to 80200 features.\n",
            "2025-11-12 22:31:11,834 INFO: [AutoFeat] With 37119 data points this new feature matrix would use about 11.91 gb of space.\n",
            "2025-11-12 22:31:11,857 INFO: [feateng] Step 1: transformation of original features\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[feateng]               0/            100 features transformed\r"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-12 22:31:13,421 INFO: [feateng] Generated 100 transformed features from 100 original features - done.\n",
            "2025-11-12 22:31:13,439 INFO: [feateng] Step 2: first combination of features\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[feateng]           19800/          19900 feature tuples combined\r"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-12 22:31:41,391 INFO: [feateng] Generated 19900 feature combinations from 19900 original feature tuples - done.\n",
            "2025-11-12 22:31:48,403 INFO: [feateng] Generated altogether 20100 new features in 2 steps\n",
            "2025-11-12 22:31:48,403 INFO: [feateng] Removing correlated features, as well as additions at the highest level\n",
            "2025-11-12 22:32:02,127 INFO: [feateng] Generated a total of 16797 additional features\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[featsel] Scaling data..."
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-12 22:32:05,442 INFO: [featsel] Feature selection run 1/5\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-12 22:32:28,843 INFO: [featsel]\t 91 initial features.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[featsel]\t Split  1/1: 108 candidate features identified.\r"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-12 22:33:17,636 INFO: \n",
            "[featsel]\t Selected  95 features after noise filtering.\n",
            "2025-11-12 22:33:17,747 INFO: [featsel] Feature selection run 2/5\n",
            "2025-11-12 22:33:42,094 INFO: [featsel]\t 91 initial features.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[featsel]\t Split  1/1: 109 candidate features identified.\r"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-12 22:34:35,149 INFO: \n",
            "[featsel]\t Selected  91 features after noise filtering.\n",
            "2025-11-12 22:34:35,237 INFO: [featsel] Feature selection run 3/5\n",
            "2025-11-12 22:35:00,366 INFO: [featsel]\t 65 initial features.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[featsel]\t Split  1/1:  76 candidate features identified.\r"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-12 22:35:53,512 INFO: \n",
            "[featsel]\t Selected  70 features after noise filtering.\n",
            "2025-11-12 22:35:53,587 INFO: [featsel] Feature selection run 4/5\n",
            "2025-11-12 22:36:15,660 INFO: [featsel]\t 70 initial features.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[featsel]\t Split  1/1: 101 candidate features identified.\r"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-12 22:37:09,111 INFO: \n",
            "[featsel]\t Selected  87 features after noise filtering.\n",
            "2025-11-12 22:37:09,180 INFO: [featsel] Feature selection run 5/5\n",
            "2025-11-12 22:37:30,277 INFO: [featsel]\t 67 initial features.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[featsel]\t Split  1/1:  77 candidate features identified.\r"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-12 22:38:21,854 INFO: \n",
            "[featsel]\t Selected  68 features after noise filtering.\n",
            "2025-11-12 22:38:21,920 INFO: [featsel] 170 features after 5 feature selection runs\n",
            "2025-11-12 22:38:33,623 INFO: [featsel] 13 features after correlation filtering\n",
            "2025-11-12 22:38:33,858 INFO: [featsel] 10 features after noise filtering\n",
            "2025-11-12 22:38:33,883 INFO: [AutoFeat] Computing 5 new features.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[AutoFeat]     3/    5 new features\r"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-12 22:38:34,724 INFO: [AutoFeat]     5/    5 new features ...done.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[AutoFeat]     4/    5 new features\r"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-12 22:38:34,741 INFO: [AutoFeat] Final dataframe with 105 feature columns (5 new).\n",
            "2025-11-12 22:38:34,742 INFO: [AutoFeat] Training final regression model.\n",
            "2025-11-12 22:38:34,860 INFO: [AutoFeat] Trained model: largest coefficients:\n",
            "2025-11-12 22:38:34,861 INFO: 3.7730978073268986\n",
            "2025-11-12 22:38:34,863 INFO: -0.000661 * childrens_transport_300m\n",
            "2025-11-12 22:38:34,863 INFO: -0.000149 * jewelry_pawnshops_300m\n",
            "2025-11-12 22:38:34,866 INFO: [AutoFeat] Final score: 0.0080\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Добавлено 5 новых фичей\n",
            "Теперь в train_data: 364 столбцов\n"
          ]
        }
      ],
      "source": [
        "# train_data2 = train_data.copy().sample(2000)\n",
        "X = train_data[num_cols]\n",
        "X = X[X.columns[~X.columns.str.endswith('_1000m')][:100]]\n",
        "y = train_data['target']\n",
        "\n",
        "# 2. Создаем и обучаем AutoFeat\n",
        "model = AutoFeatRegressor(\n",
        "    feateng_steps=2,\n",
        "    transformations=[\"log\", \"sqrt\", \"1/\", \"abs\"], \n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# Генерируем новые признаки\n",
        "X_new = model.fit_transform(X, y)\n",
        "\n",
        "# Добавляем новые признаки в исходный датафрейм\n",
        "# (исходные колонки не трогаем, добавляем только новые)\n",
        "new_features = [col for col in X_new.columns if col not in X.columns]\n",
        "for col in new_features:\n",
        "    train_data[col] = X_new[col]\n",
        "\n",
        "# Проверим результат\n",
        "print(f\"Добавлено {len(new_features)} новых фичей\")\n",
        "print(\"Теперь в train_data:\", train_data.shape[1], \"столбцов\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-12 22:39:18,425 INFO: [AutoFeat] Computing 5 new features.\n",
            "2025-11-12 22:39:18,437 INFO: [AutoFeat]     5/    5 new features ...done.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[AutoFeat]     4/    5 new features\r"
          ]
        }
      ],
      "source": [
        "X_test_new = model.transform(test_data[X.columns])\n",
        "\n",
        "for col in new_features:\n",
        "    test_data[col] = X_test_new[col]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# merge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "efFkug4DGnhK"
      },
      "outputs": [],
      "source": [
        "geo_cols = [\"traffic_300m\", \"homes_300m\", \"works_300m\", \"mean_income_300m\"]\n",
        "dem_cols = [\"female_300m\", \"male_300m\", \"age_25-34_300m\", \"employed_300m\", \"higher_education_300m\"]\n",
        "\n",
        "cluster_geo_dem = train_data.groupby(\"cluster\")[geo_cols + dem_cols].mean().add_prefix(\"cluster_mean_\")\n",
        "train_data = train_data.merge(cluster_geo_dem, on=\"cluster\", how=\"left\")\n",
        "test_data = test_data.merge(cluster_geo_dem, on=\"cluster\", how=\"left\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "HlBVdA3rx7s6"
      },
      "outputs": [],
      "source": [
        "tfidf_columns = [col for col in result.columns if col != 'id']\n",
        "\n",
        "train_with_tfidf = train_data.merge(result, on='id', how='left')\n",
        "test_with_tfidf = test_data.merge(result, on='id', how='left')\n",
        "\n",
        "train_with_tfidf[tfidf_columns] = train_with_tfidf[tfidf_columns].fillna(0)\n",
        "test_with_tfidf[tfidf_columns] = test_with_tfidf[tfidf_columns].fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "QTIkXs-ANPmx"
      },
      "outputs": [],
      "source": [
        "train_with_tfidf = add_nearest_metros(train_with_tfidf, geo_metros)\n",
        "test_with_tfidf = add_nearest_metros(test_with_tfidf, geo_metros)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DpclfCKzrRw",
        "outputId": "9ee2765d-2752-420a-8585-273b387faa70"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sum(test_with_tfidf.isna().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "uUUXYu5O3baI",
        "outputId": "d447a45b-1096-4efc-d2f0-0a72e7747f01"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train_ticket_order_300m</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>734.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4701.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1204.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>540.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2411.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37114</th>\n",
              "      <td>638.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37115</th>\n",
              "      <td>1478.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37116</th>\n",
              "      <td>1209.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37117</th>\n",
              "      <td>981.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37118</th>\n",
              "      <td>896.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>37119 rows × 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ],
            "text/plain": [
              "0         734.0\n",
              "1        4701.0\n",
              "2        1204.0\n",
              "3         540.0\n",
              "4        2411.0\n",
              "          ...  \n",
              "37114     638.0\n",
              "37115    1478.0\n",
              "37116    1209.0\n",
              "37117     981.0\n",
              "37118     896.0\n",
              "Name: train_ticket_order_300m, Length: 37119, dtype: float64"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_with_tfidf['train_ticket_order_300m']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "UssgYYfH2pK3",
        "outputId": "14ba7cc4-7e18-48a0-e42a-b5faddbb5807"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'np.array(train_with_tfidf.columns)'"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''np.array(train_with_tfidf.columns)'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQMtqsjbcioV"
      },
      "outputs": [],
      "source": [
        "# !pip install -U autogluon.tabular lightgbm\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# val split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rB6KRI_VbhYF",
        "outputId": "680bed79-cd75-4621-e8a6-2dc968041ec3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "address_count    20\n",
            "address          20\n",
            "dtype: int64\n",
            "Series([], dtype: int64)\n"
          ]
        }
      ],
      "source": [
        "train_with_tfidf = train_with_tfidf.loc[:, ~train_with_tfidf.columns.duplicated()]\n",
        "test_with_tfidf = test_with_tfidf.loc[:, ~test_with_tfidf.columns.duplicated()]\n",
        "\n",
        "\n",
        "numeric_features = []\n",
        "for col in train_with_tfidf.columns:\n",
        "    if col not in ['id', 'category', 'coordinates', 'target']:\n",
        "        if train_with_tfidf[col].dtype in ['int64', 'float64']:\n",
        "            numeric_features.append(col)\n",
        "\n",
        "categorical_features = [\n",
        "    'category', 'metro_1_name', 'metro_2_name', 'metro_3_name',\n",
        "    'name', 'address'\n",
        "]\n",
        "\n",
        "X = train_with_tfidf[numeric_features + categorical_features]\n",
        "y = train_with_tfidf['target']\n",
        "X_test = test_with_tfidf[numeric_features + categorical_features]\n",
        "\n",
        "print(X.isna().sum()[X.isna().sum() > 0])\n",
        "\n",
        "X[numeric_features] = X[numeric_features].fillna(X[numeric_features].median())\n",
        "X_test[numeric_features] = X_test[numeric_features].fillna(X[numeric_features].median())\n",
        "\n",
        "\n",
        "X[categorical_features] = X[categorical_features].fillna('unknown')\n",
        "X_test[categorical_features] = X_test[categorical_features].fillna('unknown')\n",
        "\n",
        "print(X.isna().sum()[X.isna().sum() > 0])\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.20, random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkrlx1FWmBh_"
      },
      "source": [
        "# Переходим к моделям, обучаем CatBoost, LightGBM и блендим их"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "ZkoZv7ntxNBq"
      },
      "outputs": [],
      "source": [
        "from catboost import CatBoostRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 525
        },
        "id": "MIajxnkh8REL",
        "outputId": "75b9274e-5bc2-4431-8e0d-34a848ba5f23"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Default metric period is 5 because MAE is/are not implemented for GPU\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0:\tlearn: 0.3272466\ttest: 0.3236733\tbest: 0.3236733 (0)\ttotal: 83.3ms\tremaining: 1m 23s\n",
            "100:\tlearn: 0.2551648\ttest: 0.2494563\tbest: 0.2494563 (100)\ttotal: 6.81s\tremaining: 1m\n",
            "200:\tlearn: 0.2452769\ttest: 0.2426939\tbest: 0.2426939 (200)\ttotal: 13.4s\tremaining: 53.3s\n",
            "300:\tlearn: 0.2391134\ttest: 0.2396415\tbest: 0.2396415 (300)\ttotal: 19.8s\tremaining: 46s\n",
            "400:\tlearn: 0.2342317\ttest: 0.2378991\tbest: 0.2378991 (400)\ttotal: 26.5s\tremaining: 39.6s\n",
            "500:\tlearn: 0.2297291\ttest: 0.2367584\tbest: 0.2367584 (500)\ttotal: 33.1s\tremaining: 32.9s\n",
            "600:\tlearn: 0.2262190\ttest: 0.2361290\tbest: 0.2361237 (596)\ttotal: 39.6s\tremaining: 26.3s\n",
            "700:\tlearn: 0.2228503\ttest: 0.2357366\tbest: 0.2357342 (699)\ttotal: 46.4s\tremaining: 19.8s\n",
            "800:\tlearn: 0.2199570\ttest: 0.2354506\tbest: 0.2354403 (786)\ttotal: 52.9s\tremaining: 13.1s\n",
            "900:\tlearn: 0.2171743\ttest: 0.2351277\tbest: 0.2351193 (899)\ttotal: 59.7s\tremaining: 6.57s\n",
            "999:\tlearn: 0.2142895\ttest: 0.2348912\tbest: 0.2348906 (997)\ttotal: 1m 6s\tremaining: 0us\n",
            "bestTest = 0.2348906418\n",
            "bestIteration = 997\n",
            "Shrink model to first 998 iterations.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<catboost.core.CatBoostRegressor at 0x219118d2770>"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = CatBoostRegressor(\n",
        "    # iterations=15000,\n",
        "    # learning_rate=0.01,\n",
        "    loss_function='MAE',\n",
        "    random_state=42,\n",
        "    verbose=100,\n",
        "    task_type='GPU',\n",
        "    cat_features=categorical_features, # ВСЕ ЕЩЕ НЕ ОПТИМАЛЬНЫЕ ГИПЕРПАРАМЕТРЫ!!, МОЖНО ПОЛУЧШЕ\n",
        ")\n",
        "model.fit(\n",
        "     X_train, y_train,\n",
        "    eval_set=(X_val, y_val),\n",
        "    early_stopping_rounds=100,\n",
        "    verbose=100, cat_features=categorical_features,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_a_hoNPZgeX"
      },
      "outputs": [],
      "source": [
        "# model.fit(X, y, cat_features=categorical_features, verbose=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCojuhZQZ0Gz"
      },
      "outputs": [],
      "source": [
        "# from lightgbm import LGBMRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhgtCslPd1-r"
      },
      "outputs": [],
      "source": [
        "# for col in categorical_features:\n",
        "#     X[col] = X[col].astype('category')\n",
        "# for col in categorical_features:\n",
        "#     X_test[col] = X_test[col].astype('category')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Zdb6O3RZple"
      },
      "outputs": [],
      "source": [
        "# from lightgbm import LGBMRegressor\n",
        "\n",
        "# model_lgb = LGBMRegressor(\n",
        "#     n_estimators=4000,\n",
        "#     learning_rate=0.3, #АНАЛОГИЧНО И ЗДЕСЬ\n",
        "#     objective='mae',\n",
        "#     random_state=42,\n",
        "#     verbose=-1,\n",
        "# )\n",
        "\n",
        "# model_lgb.fit(\n",
        "#     X, y,\n",
        "#     categorical_feature=categorical_features\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "lEhTZL_2RbI1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                    feature  importance\n",
            "337     category_target_enc    9.672144\n",
            "366                 tfidf_0    8.958560\n",
            "475                    name    8.098007\n",
            "471                category    5.093202\n",
            "350              name_count    4.523743\n",
            "373                 tfidf_7    3.573103\n",
            "374                 tfidf_8    3.143844\n",
            "372                 tfidf_6    2.637194\n",
            "369                 tfidf_3    2.577081\n",
            "338  mean_target_in_cluster    1.789365\n",
            "367                 tfidf_1    1.619506\n",
            "368                 tfidf_2    1.540284\n",
            "387                tfidf_21    1.447967\n",
            "379                tfidf_13    1.420164\n",
            "391                tfidf_25    1.376742\n",
            "403                tfidf_37    1.355710\n",
            "388                tfidf_22    1.287633\n",
            "375                 tfidf_9    1.150914\n",
            "370                 tfidf_4    1.140254\n",
            "400                tfidf_34    0.893852\n",
            "406                tfidf_40    0.835985\n",
            "411                tfidf_45    0.827778\n",
            "377                tfidf_11    0.809997\n",
            "376                tfidf_10    0.785897\n",
            "440                tfidf_74    0.771154\n",
            "384                tfidf_18    0.767581\n",
            "394                tfidf_28    0.741258\n",
            "393                tfidf_27    0.701491\n",
            "349           address_count    0.689006\n",
            "371                 tfidf_5    0.606159\n"
          ]
        }
      ],
      "source": [
        "feature_names = X_train.columns\n",
        "importances = model.get_feature_importance()\n",
        "\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'feature': feature_names,\n",
        "    'importance': importances\n",
        "}).sort_values(by='importance', ascending=False)\n",
        "\n",
        "print(feature_importance_df.head(30))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "ip_USHGpy0xf"
      },
      "outputs": [],
      "source": [
        "X_test = X_test.fillna('unknown')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "pcdWLOtXAYm8"
      },
      "outputs": [],
      "source": [
        "pred = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knpEN7i-jql3"
      },
      "outputs": [],
      "source": [
        "pred_q = model_lgb.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "II2mzyNskd-I"
      },
      "outputs": [],
      "source": [
        "overall_pred = pred * 0.9 + pred_q * 0.1 # тут тоже можно настраивать веса!!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "U6yEpnE3XW3r"
      },
      "outputs": [],
      "source": [
        "submission = pd.DataFrame({\n",
        "    'id': test_data['id'],\n",
        "    'target': pred\n",
        "}).to_csv('Our_final_answer_with_auto_ml.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
