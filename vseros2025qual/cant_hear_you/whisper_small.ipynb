{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9791813a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/lib/python3/dist-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/usr/lib/python3/dist-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/usr/lib/python3/dist-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import WhisperFeatureExtractor, WhisperModel, logging as hf_logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c10d3fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9a42652",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import List, Tuple, Dict, Optional, Union\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Мы используем torchaudio как основной бэкенд (лучше читает opus),\n",
    "# а librosa — как резервный вариант для подстраховки.\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b041227",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c848ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa612049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dir: /home/user/Desktop/a/train_opus\n",
      "test_dir : /home/user/Desktop/a/test_opus\n",
      "Количество тренировочных файлов: 90000\n",
      "Количество тестовых файлов: 27000\n",
      "Количество размеченных файлов (по ключам в word_bounds.json): 45000\n",
      "\n",
      "Распределение классов в train (по наличию id в word_bounds.json):\n",
      "Положительные примеры: 45000 (50.0%)\n",
      "Отрицательные примеры: 45000 (50.0%)\n",
      "\n",
      "Примеры id из train: ['0578706700521351846393642918886346925726', '3747623082454920429904507450062128040033', '5600513868334424866262280573577017975423', '6115258489815918371243570488516101842182', '7447369466826958236737655344544781920680']\n",
      "Примеры id из test : ['0499438862317083278182675188266545621622', '3161809059075456484089887853187931721551', '4695837353874467852979108224819486928786', '5125349328490651777970264902178208387036', '9188837987070080381694995888667050393849']\n",
      "Примеры POS id     : ['0588658564848284222758254070217825981013', '3743858473378128480264214418181389794712', '5598385971789991196651732353383092448330', '6112567761839195068186270360179861639418', '7438696044510009890496498732772483802988']\n"
     ]
    }
   ],
   "source": [
    "def _pick_existing(*candidates):\n",
    "    for p in candidates:\n",
    "        p = Path(p)\n",
    "        if (p / \"audio\").exists():\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "train_dir = _pick_existing(\"train_data\", \"train_opus\")\n",
    "test_dir  = _pick_existing(\"test_data\",  \"test_opus\")\n",
    "\n",
    "if train_dir is None or test_dir is None:\n",
    "    raise RuntimeError(\n",
    "        \"Не найдены папки с данными. Ожидались train_data/ или train_opus/ (и аналогично для test_*/).\"\n",
    "    )\n",
    "\n",
    "print(f\"train_dir: {train_dir.resolve()}\")\n",
    "print(f\"test_dir : {test_dir.resolve()}\")\n",
    "\n",
    "# Списки файлов\n",
    "train_files = glob(str(train_dir / \"audio\" / \"*.opus\"))\n",
    "test_files  = glob(str(test_dir  / \"audio\" / \"*.opus\"))\n",
    "\n",
    "print(f\"Количество тренировочных файлов: {len(train_files)}\")\n",
    "print(f\"Количество тестовых файлов: {len(test_files)}\")\n",
    "\n",
    "# Загрузка разметки\n",
    "wb_path = train_dir / \"word_bounds.json\"\n",
    "if not wb_path.exists():\n",
    "    raise FileNotFoundError(f\"Не найден файл разметки: {wb_path}\")\n",
    "\n",
    "with open(wb_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    word_bounds = json.load(f)\n",
    "\n",
    "print(f\"Количество размеченных файлов (по ключам в word_bounds.json): {len(word_bounds)}\")\n",
    "\n",
    "# Базовые счётчики\n",
    "train_ids = {Path(p).stem for p in train_files}\n",
    "test_ids  = {Path(p).stem for p in test_files}\n",
    "pos_ids   = set(word_bounds.keys())\n",
    "\n",
    "# Положительные — это пересечение ключей разметки с фактически существующими файлами\n",
    "pos_in_train = pos_ids & train_ids\n",
    "pos_count    = len(pos_in_train)\n",
    "neg_count    = len(train_files) - pos_count\n",
    "\n",
    "print(\"\\nРаспределение классов в train (по наличию id в word_bounds.json):\")\n",
    "print(f\"Положительные примеры: {pos_count} ({pos_count / max(len(train_files),1) * 100:.1f}%)\")\n",
    "print(f\"Отрицательные примеры: {neg_count} ({neg_count / max(len(train_files),1) * 100:.1f}%)\")\n",
    "\n",
    "# Полезные sanity-check'и\n",
    "missing_annot_ids = pos_ids - train_ids\n",
    "if missing_annot_ids:\n",
    "    print(f\"\\nПредупреждение: в разметке есть {len(missing_annot_ids)} id, \"\n",
    "          f\"для которых не найден файл в {train_dir/'audio'} (первые 5): \"\n",
    "          f\"{sorted(list(missing_annot_ids))[:5]}\")\n",
    "\n",
    "test_leak_ids = pos_ids & test_ids\n",
    "if test_leak_ids:\n",
    "    print(f\"\\nПредупреждение: обнаружены {len(test_leak_ids)} id из теста, присутствующие в word_bounds.json \"\n",
    "          f\"(первые 5): {sorted(list(test_leak_ids))[:5]}\")\n",
    "\n",
    "# Дубликаты по stem (на всякий случай)\n",
    "def _dup_stems(paths):\n",
    "    stems = [Path(p).stem for p in paths]\n",
    "    seen, dup = set(), set()\n",
    "    for s in stems:\n",
    "        if s in seen:\n",
    "            dup.add(s)\n",
    "        else:\n",
    "            seen.add(s)\n",
    "    return dup\n",
    "\n",
    "dup_train = _dup_stems(train_files)\n",
    "dup_test  = _dup_stems(test_files)\n",
    "if dup_train:\n",
    "    print(f\"\\nПредупреждение: дубликаты id в train: {len(dup_train)} (первые 5): {sorted(list(dup_train))[:5]}\")\n",
    "if dup_test:\n",
    "    print(f\"\\nПредупреждение: дубликаты id в test: {len(dup_test)} (первые 5): {sorted(list(dup_test))[:5]}\")\n",
    "\n",
    "# Небольшое превью\n",
    "def _preview(ids_set, n=5):\n",
    "    ids_list = sorted(list(ids_set))\n",
    "    random.seed(0)\n",
    "    return sorted(random.sample(ids_list, min(n, len(ids_list))))\n",
    "\n",
    "print(\"\\nПримеры id из train:\", _preview(train_ids, 5))\n",
    "print(\"Примеры id из test :\", _preview(test_ids, 5))\n",
    "print(\"Примеры POS id     :\", _preview(pos_in_train, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6fef0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio_16k(path: Union[str, Path], sr_target: int = 16000) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Загрузка аудио в 16 кГц, моно, float32 -> torch.float32.\n",
    "\n",
    "    Порядок:\n",
    "    1) torchaudio.load (часто лучший выбор для .opus при наличии ffmpeg)\n",
    "    2) librosa.load как резервный путь\n",
    "\n",
    "    Возвращает:\n",
    "        wav: Tensor [T] в диапазоне примерно [-1, 1]\n",
    "    \"\"\"\n",
    "    path = str(path)\n",
    "    try:\n",
    "        wav, sr = torchaudio.load(path)  # wav: [C, T], float32/float64/…\n",
    "        # моно\n",
    "        if wav.dim() == 2:\n",
    "            if wav.size(0) > 1:\n",
    "                wav = wav.mean(dim=0, keepdim=True)  # усредняем каналы\n",
    "            wav = wav.squeeze(0)  # -> [T]\n",
    "\n",
    "        # ресемплинг при необходимости\n",
    "        if sr != sr_target:\n",
    "            wav = torchaudio.functional.resample(wav, orig_freq=sr, new_freq=sr_target)\n",
    "\n",
    "        # гарантируем float32\n",
    "        if wav.dtype != torch.float32:\n",
    "            wav = wav.float()\n",
    "        return wav\n",
    "    except Exception:\n",
    "        # librosa (например, если нет ffmpeg-бэкенда у torchaudio)\n",
    "        y, sr = librosa.load(path, sr=sr_target, mono=True)\n",
    "        wav = torch.from_numpy(y.astype(np.float32))\n",
    "        return wav\n",
    "\n",
    "\n",
    "def ensure_length(wav: torch.Tensor, target_len: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Приводим сигнал к заданной длине target_len с помощью паддинга нулями или обрезания.\n",
    "    Важно для батчинга: у всех сегментов одинаковая длина.\n",
    "    \"\"\"\n",
    "    T = wav.numel()\n",
    "    if T == target_len:\n",
    "        return wav\n",
    "    if T > target_len:\n",
    "        return wav[:target_len]\n",
    "    # паддинг справа\n",
    "    pad = target_len - T\n",
    "    return torch.nn.functional.pad(wav, (0, pad))\n",
    "\n",
    "\n",
    "def pick_positive_window(\n",
    "    T: int,\n",
    "    sr: int,\n",
    "    seg_size: int,\n",
    "    bounds: Tuple[float, float],\n",
    "    context_frac: float = 0.5,\n",
    ") -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Возвращает [left, right) индексы окна длины seg_size так, чтобы целевая фраза попала внутрь,\n",
    "    и при этом оставалось немного контекста до/после (задается долей context_frac).\n",
    "\n",
    "    Идея:\n",
    "    - Есть интервал фразы [t0, t1] в секундах. Его длина Lp = (t1 - t0) * sr.\n",
    "    - Мы хотим поместить этот интервал в окно длины seg_size, желательно не \"впритык\".\n",
    "    - Сначала вычисляем допустимый диапазон для левого края окна так, чтобы фраза полностью влезла.\n",
    "    - Затем случайно сдвигаем окно в этом диапазоне (даёт разнообразие).\n",
    "    \"\"\"\n",
    "    t0, t1 = bounds\n",
    "    # Границы фразы в сэмплах\n",
    "    p0 = int(round(t0 * sr))\n",
    "    p1 = int(round(t1 * sr))\n",
    "    p0 = max(0, min(T, p0))\n",
    "    p1 = max(0, min(T, p1))\n",
    "    pos_len = max(1, p1 - p0)\n",
    "\n",
    "    # Если фраза длиннее окна, берём центральный фрагмент фразы\n",
    "    if pos_len >= seg_size:\n",
    "        center = (p0 + p1) // 2\n",
    "        left = max(0, center - seg_size // 2)\n",
    "        right = min(T, left + seg_size)\n",
    "        left = right - seg_size\n",
    "        return left, right\n",
    "\n",
    "    # Иначе фраза короче окна -> разместим её внутри окна с контекстом\n",
    "    free = seg_size - pos_len\n",
    "\n",
    "    # Доля контекста до/после. Например, context_frac=0.5 -> симметричный контекст\n",
    "    # но мы слегка рандомизируем долю слева/справа\n",
    "    alpha = np.clip(np.random.normal(loc=context_frac, scale=0.15), 0.0, 1.0)\n",
    "    left_ctx = int(alpha * free)\n",
    "    right_ctx = free - left_ctx\n",
    "\n",
    "    left = p0 - left_ctx\n",
    "    right = p1 + right_ctx\n",
    "\n",
    "    # Если окно \"вылезает\" за аудио — подвинем\n",
    "    if left < 0:\n",
    "        shift = -left\n",
    "        left = 0\n",
    "        right = min(T, right + shift)\n",
    "    if right > T:\n",
    "        shift = right - T\n",
    "        right = T\n",
    "        left = max(0, left - shift)\n",
    "\n",
    "    # На всякий случай — фиксированная длина\n",
    "    if right - left != seg_size:\n",
    "        right = min(T, left + seg_size)\n",
    "        left = right - seg_size\n",
    "        left = max(0, left)\n",
    "    return left, right\n",
    "\n",
    "\n",
    "def pick_negative_window(T: int, seg_size: int) -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Простой случай для отрицательного окна: равномерно выбираем позицию,\n",
    "    если сигнал короче seg_size — окно начнётся с 0, а остаток добьётся нулями.\n",
    "    \"\"\"\n",
    "    if T <= seg_size:\n",
    "        return 0, min(T, seg_size)\n",
    "    left = random.randint(0, T - seg_size)\n",
    "    right = left + seg_size\n",
    "    return left, right\n",
    "\n",
    "\n",
    "class KWSDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Базовый датасет для KWS.\n",
    "\n",
    "    Ожидаемые входы:\n",
    "        pos_items: список положительных примеров [(path, (start_sec, end_sec)), ...]\n",
    "        neg_items: список отрицательных примеров [(path, None), ...] или [(path, ()), ...]\n",
    "    где:\n",
    "        - path: путь к .opus\n",
    "        - (start_sec, end_sec): границы фразы из word_bounds.json для данного файла\n",
    "          (если в файле несколько фрагментов, можно передавать список — см. ниже расширение)\n",
    "\n",
    "    Параметры:\n",
    "        seg_size_samples: длина сегмента в сэмплах (для 1 сек при 16кГц: 16000)\n",
    "        sr: частота дискретизации для загрузки и сегментации (обычно 16000)\n",
    "        mix_posneg: если True — формируем единый пул и даем DataLoader'у управлять shuffle;\n",
    "                    если False — можно реализовать собственную балансировку.\n",
    "        seed: сид для воспроизводимости.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        pos_items: List[Tuple[Union[str, Path], Tuple[float, float]]],\n",
    "        neg_items: List[Tuple[Union[str, Path], Optional[Tuple[float, float]]]],\n",
    "        seg_size_samples: int,\n",
    "        sr: int = 16000,\n",
    "        mix_posneg: bool = True,\n",
    "        seed: Optional[int] = None,\n",
    "        allow_negative_inside_positive: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.sr = sr\n",
    "        self.seg_size = int(seg_size_samples)\n",
    "        self.rng = random.Random(seed)\n",
    "\n",
    "        # Нормализуем вход: приводим пути к Path, фильтруем битые записи\n",
    "        def _normalize(items, is_pos: bool):\n",
    "            out = []\n",
    "            for it in items:\n",
    "                if isinstance(it, (list, tuple)) and len(it) == 2:\n",
    "                    p, b = it\n",
    "                else:\n",
    "                    # допускаем формат просто (path,) для отрицательных\n",
    "                    p, b = it, None\n",
    "                p = Path(p)\n",
    "                if not p.exists():\n",
    "                    # лучше явно сигнализировать, но для демонстрационного ноутбука мы молча пропустим\n",
    "                    continue\n",
    "                if is_pos and (b is None or len(b) != 2):\n",
    "                    # у положительного примера обязательно должны быть границы\n",
    "                    continue\n",
    "                out.append((p, b))\n",
    "            return out\n",
    "\n",
    "        self.pos_items = _normalize(pos_items, is_pos=True)\n",
    "        self.neg_items = _normalize(neg_items, is_pos=False)\n",
    "\n",
    "        if mix_posneg:\n",
    "            # формируем единый пул (path, bounds, label)\n",
    "            self.samples = [(p, b, 1) for p, b in self.pos_items] + [(p, None, 0) for p, _ in self.neg_items]\n",
    "        else:\n",
    "            # можно хранить раздельно и реализовать свою стратегию балансировки в __getitem__\n",
    "            self.samples = [(p, b, 1) for p, b in self.pos_items] + [(p, None, 0) for p, _ in self.neg_items]\n",
    "\n",
    "        # Флаг: разрешать ли извлекать отрицательные окна из «пустых зон» положительных файлов\n",
    "        # (полезно при дефиците отрицательных примеров)\n",
    "        self.allow_negative_inside_positive = allow_negative_inside_positive\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def _get_segment(self, wav: torch.Tensor, label: int, bounds: Optional[Tuple[float, float]]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Возвращает сегмент фиксированной длины для данного wav и метки.\n",
    "        Для label==1 — гарантируем попадание фразы в окно.\n",
    "        Для label==0 — равномерный выбор окна.\n",
    "\n",
    "        Если wav слишком короткий — дополняем нулями.\n",
    "        \"\"\"\n",
    "        T = wav.numel()\n",
    "\n",
    "        # Если аудио пустое (редкий случай), вернем просто нули\n",
    "        if T == 0:\n",
    "            return torch.zeros(self.seg_size, dtype=torch.float32)\n",
    "\n",
    "        if label == 1 and bounds is not None:\n",
    "            left, right = pick_positive_window(T=T, sr=self.sr, seg_size=self.seg_size, bounds=bounds)\n",
    "        else:\n",
    "            left, right = pick_negative_window(T=T, seg_size=self.seg_size)\n",
    "\n",
    "        seg = wav[left:right]\n",
    "        seg = ensure_length(seg, self.seg_size)\n",
    "        return seg\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        \"\"\"\n",
    "        Возвращает:\n",
    "            segment: Tensor [seg_size] float32\n",
    "            label:   int (0/1)\n",
    "            aux:     словарь с технической информацией (путь, длительность и т.п.) — полезно для отладки\n",
    "        \"\"\"\n",
    "        index = index % len(self.samples)\n",
    "        path, bounds, label = self.samples[index]\n",
    "\n",
    "        # Загрузка и нормализация аудио\n",
    "        wav = load_audio_16k(path, sr_target=self.sr)\n",
    "\n",
    "        # Нормализация амплитуды по RMS/peak может помочь (опционально).\n",
    "        # Для простоты — легкий нормировщик по пику с защитой от деления на 0.\n",
    "        peak = wav.abs().max().item()\n",
    "        if peak > 0:\n",
    "            wav = wav / peak\n",
    "\n",
    "        # Сегмент\n",
    "        segment = self._get_segment(wav, label, bounds)\n",
    "        if segment is None:\n",
    "            print(\"WARN: segment is None!\", path, bounds, label)\n",
    "            segment = torch.zeros(self.seg_size, dtype=torch.float32)\n",
    "\n",
    "        aux = {\n",
    "            \"path\": str(path),\n",
    "            \"label\": label,\n",
    "            \"duration_sec\": len(wav) / float(self.sr),\n",
    "            \"bounds\": bounds,\n",
    "        }\n",
    "        return segment, label, aux\n",
    "\n",
    "\n",
    "# Сформируем pos/neg списки для датасета.\n",
    "def build_pos_neg_lists(train_files: List[str], word_bounds: Dict[str, List[float]]) \\\n",
    "        -> Tuple[List[Tuple[str, Tuple[float, float]]], List[Tuple[str, None]]]:\n",
    "    pos_items, neg_items = [], []\n",
    "    for f in train_files:\n",
    "        fid = Path(f).stem\n",
    "        if fid in word_bounds:\n",
    "            start, end = word_bounds[fid]\n",
    "            pos_items.append((f, (float(start), float(end))))\n",
    "        else:\n",
    "            neg_items.append((f, None))\n",
    "    return pos_items, neg_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a6544a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_items, neg_items = build_pos_neg_lists(train_files, word_bounds)\n",
    "\n",
    "# seg_size_samples = 16000  # 1 сек при 16 кГц\n",
    "# ds = KWSDataset(\n",
    "#     pos_items=pos_items,\n",
    "#     neg_items=neg_items,\n",
    "#     seg_size_samples=seg_size_samples,\n",
    "#     sr=16000,\n",
    "#     mix_posneg=True,\n",
    "#     seed=42,\n",
    "# )\n",
    "\n",
    "# def collate_fn(batch):\n",
    "#     segments, labels, aux = zip(*batch)\n",
    "#     segments = torch.stack(segments, dim=0)          # [B, T]\n",
    "#     labels = torch.tensor(labels, dtype=torch.long)  # [B]\n",
    "#     return segments, labels, aux\n",
    "\n",
    "# loader = DataLoader(ds, batch_size=32, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "# xb, yb, auxb = next(iter(loader))\n",
    "# print(xb.shape, yb.shape, auxb[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbe615a",
   "metadata": {},
   "source": [
    "# говнокод от гпт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85b74f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhisperBinaryClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_whisper: str = \"openai/whisper-small\"):\n",
    "        super().__init__()\n",
    "        # Загружаем WhisperModel (encoder+decoder). Мы будем использовать encoder outputs.\n",
    "        self.whisper = WhisperModel.from_pretrained(pretrained_whisper)\n",
    "        # Размер скрытого слоя энкодера\n",
    "        enc_hidden = self.whisper.config.d_model  # usually 768/1024/... depending on size\n",
    "        # Глобальный pooling -> логит\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)  # будем применять к [B, C, T] -> [B, C, 1]\n",
    "        self.classifier = nn.Linear(enc_hidden, 1)\n",
    "\n",
    "    def forward(self, input_features, attention_mask=None):\n",
    "        # Используем только encoder\n",
    "        encoder_outputs = self.whisper.encoder(input_features=input_features, attention_mask=attention_mask)\n",
    "        pooled = encoder_outputs.last_hidden_state.mean(dim=1)  # [B, hidden]\n",
    "        logits = self.classifier(pooled).squeeze(-1)  # [B]\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d9abc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_feature_extractor(model_name=\"openai/whisper-small\"):\n",
    "    fe = WhisperFeatureExtractor.from_pretrained(model_name)\n",
    "    return fe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a18ee7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_whisper(batch, feature_extractor):\n",
    "    \"\"\"\n",
    "    batch: from your KWSDataset -> (segments [T], label, aux)\n",
    "    feature_extractor: WhisperFeatureExtractor (transformers)\n",
    "    Возвращает input_features Tensor [B, seq_len, feat_dim], labels Tensor [B]\n",
    "    \"\"\"\n",
    "    segments, labels, aux = zip(*batch)\n",
    "    # segments: list of tensors [T]\n",
    "    # преобразуем в numpy float32 и в список\n",
    "    segs = [s.numpy() if isinstance(s, torch.Tensor) else s for s in segments]\n",
    "    # feature_extractor ожидает list[np.ndarray] и вернёт 'input_features' (лог-мел)\n",
    "    # return_tensors=None -> вернёт списки/np, но мы хотим тензор -> return_tensors='pt'\n",
    "    feats = feature_extractor(segs, sampling_rate=feature_extractor.sampling_rate, return_tensors=\"pt\").input_features\n",
    "    labels = torch.tensor(labels, dtype=torch.float32)\n",
    "    return feats, labels, aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b3fe35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_harmonic(y_true: torch.Tensor, y_prob: torch.Tensor, thr: float = 0.5):\n",
    "    \"\"\"\n",
    "    Возвращает FRR, FAR и score = harmonic_mean(1-FRR, 1-FAR)\n",
    "    y_true: {0,1}\n",
    "    y_prob: probabilities or logits (будем применять sigmoid)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        probs = torch.sigmoid(y_prob) if y_prob.max() > 1.0 or y_prob.min() < 0 else torch.clamp(y_prob, 0.0, 1.0) if y_prob.max() <= 1.0 else torch.sigmoid(y_prob)\n",
    "        preds = (probs >= thr).long()\n",
    "        y_true = y_true.long()\n",
    "        TP = int(((preds == 1) & (y_true == 1)).sum().item())\n",
    "        FN = int(((preds == 0) & (y_true == 1)).sum().item())\n",
    "        FP = int(((preds == 1) & (y_true == 0)).sum().item())\n",
    "        TN = int(((preds == 0) & (y_true == 0)).sum().item())\n",
    "        NUM_POS = TP + FN\n",
    "        NUM_NEG = FP + TN\n",
    "        FRR = FN / NUM_POS if NUM_POS > 0 else 0.0\n",
    "        FAR = FP / NUM_NEG if NUM_NEG > 0 else 0.0\n",
    "        one_minus_frr = 1.0 - FRR\n",
    "        one_minus_far = 1.0 - FAR\n",
    "        # harmonic mean safe\n",
    "        if (one_minus_frr + one_minus_far) == 0:\n",
    "            score = 0.0\n",
    "        else:\n",
    "            score = 2 * one_minus_frr * one_minus_far / (one_minus_frr + one_minus_far)\n",
    "        return {\"TP\": TP, \"FP\": FP, \"FN\": FN, \"TN\": TN, \"FRR\": FRR, \"FAR\": FAR, \"score\": score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b93be4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(\n",
    "    model: nn.Module,\n",
    "    feature_extractor,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: Optional[DataLoader],\n",
    "    device: torch.device,\n",
    "    epochs: int = 5,\n",
    "    lr: float = 1e-5,\n",
    "    grad_accum_steps: int = 1,\n",
    "    save_path: str = \"./whisper_kws_ckpt.pt\",\n",
    "):\n",
    "    model.to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(device.type == \"cuda\"))\n",
    "    best_score = -1.0\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        n_batches = 0\n",
    "        optimizer.zero_grad()\n",
    "        for step, (feats, labels, aux) in tqdm(enumerate(train_loader, start=1), total=len(train_loader)):\n",
    "            feats = feats.to(device)  # [B, seq_len, feat_dim]\n",
    "            labels = labels.to(device)\n",
    "            with torch.cuda.amp.autocast(enabled=(device.type == \"cuda\")):\n",
    "                logits = model(feats)  # [B]\n",
    "                loss = criterion(logits, labels)\n",
    "                loss = loss / grad_accum_steps\n",
    "            scaler.scale(loss).backward()\n",
    "            total_loss += loss.item() * grad_accum_steps\n",
    "            n_batches += 1\n",
    "\n",
    "            if step % grad_accum_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        avg_loss = total_loss / n_batches if n_batches > 0 else 0.0\n",
    "        print(f\"Epoch {epoch} train_loss={avg_loss:.6f}\")\n",
    "\n",
    "        # Валидация\n",
    "        if val_loader is not None:\n",
    "            model.eval()\n",
    "            all_logits = []\n",
    "            all_labels = []\n",
    "            with torch.no_grad():\n",
    "                for feats, labels, aux in tqdm(val_loader):\n",
    "                    feats = feats.to(device)\n",
    "                    labels = labels.to(device)\n",
    "                    logits = model(feats)\n",
    "                    all_logits.append(logits.cpu())\n",
    "                    all_labels.append(labels.cpu())\n",
    "            all_logits = torch.cat(all_logits, dim=0)\n",
    "            all_labels = torch.cat(all_labels, dim=0)\n",
    "            metrics = compute_metrics_harmonic(all_labels, torch.sigmoid(all_logits))\n",
    "            print(f\"VAL metrics: score={metrics['score']:.4f} FRR={metrics['FRR']:.4f} FAR={metrics['FAR']:.4f} TP={metrics['TP']} FP={metrics['FP']} FN={metrics['FN']} TN={metrics['TN']}\")\n",
    "\n",
    "            if metrics[\"score\"] > best_score:\n",
    "                best_score = metrics[\"score\"]\n",
    "                # сохраняем чекпоинт\n",
    "                torch.save({\"epoch\": epoch, \"model_state\": model.state_dict(), \"optimizer_state\": optimizer.state_dict()}, save_path)\n",
    "                print(f\"Saved best checkpoint with score={best_score:.4f} -> {save_path}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5934c21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_AUDIO_DIR = \"./train_opus/audio\"  # поменяй при необходимости\n",
    "WORD_BOUNDS_PATH = \"./train_opus/word_bounds.json\"\n",
    "PRETRAINED_WHISPER = \"openai/whisper-small\"  # можно 'small', 'base' и т.д.\n",
    "BATCH_SIZE = 32\n",
    "SEG_SECONDS = 1.0\n",
    "SEG_SAMPLES = int(16000 * SEG_SECONDS)\n",
    "EPOCHS = 3\n",
    "LR = 3e-5\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cfea8f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(WORD_BOUNDS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    word_bounds = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e2d513c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_items, neg_items = build_pos_neg_lists(train_files, word_bounds)\n",
    "\n",
    "ds = KWSDataset(\n",
    "    pos_items=pos_items,\n",
    "    neg_items=neg_items,\n",
    "    seg_size_samples=SEG_SAMPLES,\n",
    "    sr=16000,\n",
    "    mix_posneg=True,\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7833227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c49c142",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(ds)\n",
    "idxs = list(range(n))\n",
    "random.shuffle(idxs)\n",
    "n_val = max(1, int(0.1 * n))\n",
    "val_idxs = set(idxs[:n_val])\n",
    "train_idx_list = [i for i in idxs if i not in val_idxs]\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "ds_train = Subset(ds, train_idx_list)\n",
    "ds_val = Subset(ds, list(val_idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15665e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = prepare_feature_extractor(PRETRAINED_WHISPER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae9d9380",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda batch: collate_fn_whisper(batch, feature_extractor)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True, collate_fn=f, num_workers=16)\n",
    "val_loader = DataLoader(ds_val, batch_size=BATCH_SIZE, shuffle=False, collate_fn=f, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ea18f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10953/762828574.py:15: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(device.type == \"cuda\"))\n",
      "  0%|          | 0/2532 [00:00<?, ?it/s]/tmp/ipykernel_10953/762828574.py:26: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device.type == \"cuda\")):\n",
      "100%|██████████| 2532/2532 [40:58<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train_loss=0.059277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [04:34<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL metrics: score=0.9883 FRR=0.0096 FAR=0.0138 TP=4455 FP=62 FN=43 TN=4440\n",
      "Saved best checkpoint with score=0.9883 -> ./whisper_kws_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2532/2532 [40:58<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 train_loss=0.039557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [04:34<00:00,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL metrics: score=0.9816 FRR=0.0225 FAR=0.0142 TP=4397 FP=64 FN=101 TN=4438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2532/2532 [40:58<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 train_loss=0.034025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [04:34<00:00,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL metrics: score=0.9880 FRR=0.0091 FAR=0.0149 TP=4457 FP=67 FN=41 TN=4435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = WhisperBinaryClassifier(pretrained_whisper=PRETRAINED_WHISPER)\n",
    "\n",
    "trained_model = train_loop(\n",
    "    model=model,\n",
    "    feature_extractor=feature_extractor,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=DEVICE,\n",
    "    epochs=EPOCHS,\n",
    "    lr=LR,\n",
    "    grad_accum_steps=1,\n",
    "    save_path=\"./whisper_kws_best.pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71d3bfc",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c534f9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "252ca9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PATH = \"./test_opus/audio\"\n",
    "files = [f for f in os.listdir(TEST_PATH) if f.endswith(\".opus\") and not f.startswith(\"._\")]\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, path, sr=16000):\n",
    "        self.files = [f for f in os.listdir(path) if f.endswith(\".opus\") and not f.startswith(\"._\")]\n",
    "        self.path = path\n",
    "        self.sr = sr\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.files[idx]\n",
    "        file_id = os.path.splitext(fname)[0]\n",
    "        waveform, sr = torchaudio.load(os.path.join(self.path, fname))\n",
    "        waveform = waveform.mean(dim=0)\n",
    "        if sr != self.sr:\n",
    "            waveform = torchaudio.functional.resample(waveform, sr, self.sr)\n",
    "        return file_id, waveform\n",
    "\n",
    "def collate_fn(batch):\n",
    "    ids, waves = zip(*batch)\n",
    "    feats = feature_extractor([w.numpy() for w in waves], sampling_rate=16_000, return_tensors=\"pt\").input_features\n",
    "    return ids, feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8995032b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AudioDataset(TEST_PATH)\n",
    "loader = DataLoader(dataset, batch_size=16, collate_fn=collate_fn, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d504499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6fb4911b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1688/1688 [14:11<00:00,  1.98it/s]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "all_logits = []\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for ids, feats in tqdm(loader):\n",
    "        feats = feats.to(DEVICE)\n",
    "        batch_logits = model(feats).cpu().numpy().ravel()\n",
    "        all_logits.extend(batch_logits)\n",
    "        results.extend(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0af21872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    13554\n",
       "0    13446\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"id\": results,\n",
    "    \"logit\": all_logits\n",
    "})\n",
    "\n",
    "# Теперь можно менять порог сколько угодно раз:\n",
    "threshold = 0.07\n",
    "df[\"label\"] = (torch.sigmoid(torch.tensor(df[\"logit\"])) > threshold).int().numpy()\n",
    "df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "73ccba33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Превращаем в DataFrame с логитами\n",
    "\n",
    "\n",
    "df[[\"id\", \"label\"]].to_csv(\"sub4.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
