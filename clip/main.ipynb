{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git 'C:\\Users\\User\\AppData\\Local\\Temp\\pip-req-build-w_ti9mj6'\n",
      "\n",
      "[notice] A new release of pip is available: 24.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to c:\\users\\user\\appdata\\local\\temp\\pip-req-build-w_ti9mj6\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: onnxruntime in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.18.0)\n",
      "Requirement already satisfied: onnx-simplifier in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.4.36)\n",
      "Requirement already satisfied: ftfy in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from clip==1.0) (6.3.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from clip==1.0) (24.1)\n",
      "Requirement already satisfied: regex in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from clip==1.0) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from clip==1.0) (4.66.4)\n",
      "Requirement already satisfied: torch in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from clip==1.0) (2.3.1+cu118)\n",
      "Requirement already satisfied: torchvision in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from clip==1.0) (0.18.1+cu118)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from onnxruntime) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from onnxruntime) (24.3.25)\n",
      "Requirement already satisfied: numpy>=1.21.6 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from onnxruntime) (1.26.4)\n",
      "Requirement already satisfied: protobuf in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from onnxruntime) (5.28.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from onnxruntime) (1.12)\n",
      "Requirement already satisfied: onnx in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from onnx-simplifier) (1.17.0)\n",
      "Requirement already satisfied: rich in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from onnx-simplifier) (13.9.4)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from coloredlogs->onnxruntime) (10.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from ftfy->clip==1.0) (0.2.13)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rich->onnx-simplifier) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from rich->onnx-simplifier) (2.18.0)\n",
      "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from rich->onnx-simplifier) (4.12.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->onnxruntime) (1.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch->clip==1.0) (3.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch->clip==1.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch->clip==1.0) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch->clip==1.0) (2024.2.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch->clip==1.0) (2021.4.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision->clip==1.0) (10.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from tqdm->clip==1.0) (0.4.6)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime) (3.5.4)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->onnx-simplifier) (0.1.2)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->clip==1.0) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->clip==1.0) (2021.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch->clip==1.0) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/openai/CLIP.git onnxruntime onnx-simplifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import time\n",
    "import torch\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "from onnxsim import simplify\n",
    "\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIP_BACKBONE = 'RN50'\n",
    "CLIP_ONNX_EXPORT_PATH = 'clip_resnet.onnx'\n",
    "CLIP_ONNX_EXPORT_PATH_SIMP = 'clip_resnet_simplified.onnx'\n",
    "\n",
    "ONNX_INPUT_NAMES = [\"IMAGE\", \"TEXT\"]\n",
    "ONNX_OUTPUT_NAMES = [\"LOGITS_PER_IMAGE\", \"LOGITS_PER_TEXT\"]\n",
    "ONNX_DYNAMIC_AXES = {\n",
    "    \"IMAGE\": {\n",
    "        0: \"image_batch_size\",\n",
    "    },\n",
    "    \"TEXT\": {\n",
    "        0: \"text_batch_size\",\n",
    "    },\n",
    "    \"LOGITS_PER_IMAGE\": {\n",
    "        0: \"image_batch_size\",\n",
    "        1: \"text_batch_size\",\n",
    "    },\n",
    "    \"LOGITS_PER_TEXT\": {\n",
    "        0: \"text_batch_size\",\n",
    "        1: \"image_batch_size\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_mean_time_no_warmup(\n",
    "    func, \n",
    "    func_inputs, \n",
    "    num_iters=250\n",
    ") -> float:\n",
    "    start_time = time.perf_counter()\n",
    "    for _ in range(num_iters):\n",
    "        func(*func_inputs)\n",
    "    return (time.perf_counter() - start_time) / num_iters\n",
    "\n",
    "def load_clip(backbone='RN50', device='cpu') -> Tuple[clip.model.CLIP, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "    pytorch_model, pre = clip.load(backbone)\n",
    "    \n",
    "    # Переместим модель на нужное устройство\n",
    "    pytorch_model = pytorch_model.to(device)\n",
    "    \n",
    "    # Генерируем dummy inputs и перемещаем их на то же устройство\n",
    "    npx = pytorch_model.visual.input_resolution\n",
    "    dummy_image = torch.randn(10, 3, npx, npx).to(device)\n",
    "    dummy_texts = clip.tokenize([\"quick brown fox\", \"lorem ipsum\"]).to(device)\n",
    "    \n",
    "    return pytorch_model, (dummy_image, dummy_texts)\n",
    "\n",
    "def export_onnx(\n",
    "    model, \n",
    "    inputs, \n",
    "    input_names,\n",
    "    output_names,\n",
    "    dynamic_axes,\n",
    "    export_path\n",
    ") -> None:\n",
    "    torch.onnx.export(\n",
    "        model=model, \n",
    "        args=inputs, \n",
    "        f=export_path, \n",
    "        export_params=True,\n",
    "        input_names=input_names,\n",
    "        output_names=output_names,\n",
    "        opset_version=14,\n",
    "        dynamic_axes=dynamic_axes\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:5504: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\onnx\\symbolic_opset9.py:5858: UserWarning: Exporting aten::index operator of advanced indexing in opset 14 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pytorch_model, dummy_input = load_clip(backbone=CLIP_BACKBONE, device=device)\n",
    "pytorch_model.eval()\n",
    "pytorch_model.float()\n",
    "\n",
    "export_onnx(\n",
    "    model=pytorch_model,\n",
    "    inputs=dummy_input,\n",
    "    input_names=ONNX_INPUT_NAMES,\n",
    "    output_names=ONNX_OUTPUT_NAMES,\n",
    "    dynamic_axes=ONNX_DYNAMIC_AXES,\n",
    "    export_path=CLIP_ONNX_EXPORT_PATH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run checks\n",
    "onnx_model = onnx.load(CLIP_ONNX_EXPORT_PATH)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "\n",
    "# run additional checks and simplify\n",
    "model_simp, check = simplify(onnx_model, skip_fuse_bn=True)\n",
    "assert check, \"Simplified ONNX model could not be validated\"\n",
    "onnx.save(model_simp, CLIP_ONNX_EXPORT_PATH_SIMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_sess = ort.InferenceSession(CLIP_ONNX_EXPORT_PATH_SIMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Output 0:\n",
      " tensor([[14.0831, 18.9359],\n",
      "        [13.6810, 18.7863],\n",
      "        [14.5861, 18.8607],\n",
      "        [14.2820, 19.0347],\n",
      "        [14.0733, 18.8717],\n",
      "        [14.4109, 19.1839],\n",
      "        [14.3502, 19.3408],\n",
      "        [14.3873, 18.8750],\n",
      "        [14.2595, 19.1647],\n",
      "        [14.2893, 19.0929]])\n",
      "ONNX Output 0:\n",
      " tensor([[14.0844, 18.9358],\n",
      "        [13.6807, 18.7858],\n",
      "        [14.5882, 18.8621],\n",
      "        [14.2823, 19.0324],\n",
      "        [14.0748, 18.8727],\n",
      "        [14.4093, 19.1830],\n",
      "        [14.3506, 19.3406],\n",
      "        [14.3906, 18.8766],\n",
      "        [14.2585, 19.1637],\n",
      "        [14.2909, 19.0933]])\n",
      "Difference (absolute):\n",
      " tensor(0.0033)\n",
      "PyTorch Output 1:\n",
      " tensor([[14.0831, 13.6810, 14.5861, 14.2820, 14.0733, 14.4109, 14.3502, 14.3873,\n",
      "         14.2595, 14.2893],\n",
      "        [18.9359, 18.7863, 18.8607, 19.0347, 18.8717, 19.1839, 19.3408, 18.8750,\n",
      "         19.1647, 19.0929]])\n",
      "ONNX Output 1:\n",
      " tensor([[14.0844, 13.6807, 14.5882, 14.2823, 14.0748, 14.4093, 14.3506, 14.3906,\n",
      "         14.2585, 14.2909],\n",
      "        [18.9358, 18.7858, 18.8621, 19.0324, 18.8727, 19.1830, 19.3406, 18.8766,\n",
      "         19.1637, 19.0933]])\n",
      "Difference (absolute):\n",
      " tensor(0.0033)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    pytorch_output = pytorch_model(*dummy_input)\n",
    "\n",
    "onnx_output = ort_sess.run(\n",
    "    ONNX_OUTPUT_NAMES, \n",
    "    {\n",
    "        \"IMAGE\": dummy_input[0].cpu().numpy(), \n",
    "        \"TEXT\": dummy_input[1].cpu().numpy()\n",
    "    }\n",
    ")\n",
    "\n",
    "# Сравним и выведем значения\n",
    "for i, (pt_pred, onnx_pred) in enumerate(zip(pytorch_output, onnx_output)):\n",
    "    pt_pred_cpu = pt_pred.cpu()  # Переводим выходной тензор PyTorch на CPU для сравнения\n",
    "    onnx_pred_tensor = torch.tensor(onnx_pred)  # Преобразуем выходные данные ONNX в тензор\n",
    "    \n",
    "    # Вывод для отладки\n",
    "    print(f\"PyTorch Output {i}:\\n\", pt_pred_cpu)\n",
    "    print(f\"ONNX Output {i}:\\n\", onnx_pred_tensor)\n",
    "    print(f\"Difference (absolute):\\n\", torch.abs(pt_pred_cpu - onnx_pred_tensor).max())\n",
    "\n",
    "    # Проверка с увеличенной погрешностью\n",
    "    assert torch.allclose(pt_pred_cpu, onnx_pred_tensor, atol=1e-4, rtol=1e-3), f\"Outputs differ at index {i}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch mean time: 0.033 sec\n",
      "ONNX Runtime mean time: 0.293 sec\n",
      "Boost from PT -> ONNX (%) -783.0\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    pytorch_mean_time = measure_mean_time_no_warmup(func=pytorch_model, func_inputs=dummy_input)\n",
    "onnx_runtime_mean_time = measure_mean_time_no_warmup(func=ort_sess.run, func_inputs=([\"LOGITS_PER_IMAGE\", \"LOGITS_PER_TEXT\"], {\"IMAGE\": dummy_input[0].cpu().numpy(), \"TEXT\": dummy_input[1].cpu().numpy()}))\n",
    "\n",
    "print(f'PyTorch mean time: {round(pytorch_mean_time, 3)} sec\\nONNX Runtime mean time: {round(onnx_runtime_mean_time, 3)} sec\\nBoost from PT -> ONNX (%) {100*round(1 - onnx_runtime_mean_time/pytorch_mean_time, 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# что мне с этим добром делать"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка изображений и текстов\n",
    "Для модели CLIP входными данными являются изображения и текст. Вам нужно убедиться, что они в правильном формате.\n",
    "\n",
    "Изображения должны быть преобразованы в тензоры PyTorch размера (batch_size, 3, H, W), где H и W — это разрешение, которое ожидает CLIP (обычно 224x224 или 336x336 для предобученных моделей).\n",
    "Тексты должны быть токенизированы в формат, поддерживаемый CLIP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Преобразование данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "# Подготовка собственных данных\n",
    "image_path = 'path_to_your_image.jpg'\n",
    "text_inputs = [\"это мой текст\", \"еще один пример текста\"]\n",
    "\n",
    "# Загружаем и подготавливаем изображение\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "image_input = preprocess(image).unsqueeze(0).to(device)  # Добавляем измерение для batch и отправляем на device\n",
    "\n",
    "# Токенизация текстов для CLIP\n",
    "text_input = clip.tokenize(text_inputs).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## инференс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch инференс\n",
    "with torch.no_grad():\n",
    "    pytorch_output = pytorch_model(image_input, text_input)\n",
    "\n",
    "# ONNX инференс\n",
    "onnx_output = ort_sess.run(\n",
    "    ONNX_OUTPUT_NAMES, \n",
    "    {\n",
    "        \"IMAGE\": image_input.cpu().numpy(),\n",
    "        \"TEXT\": text_input.cpu().numpy()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## работа с результатами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразуем результаты в тензоры для дальнейшего использования\n",
    "pytorch_logits_image, pytorch_logits_text = pytorch_output\n",
    "onnx_logits_image, onnx_logits_text = [torch.tensor(out) for out in onnx_output]\n",
    "\n",
    "# Расчёт сходства (например, скалярное произведение или другой метод) между изображениями и текстами\n",
    "similarity_scores_pytorch = pytorch_logits_image @ pytorch_logits_text.T\n",
    "similarity_scores_onnx = onnx_logits_image @ onnx_logits_text.T\n",
    "\n",
    "# Выводим или используем результаты\n",
    "print(\"Сходство для PyTorch модели:\\n\", similarity_scores_pytorch)\n",
    "print(\"Сходство для ONNX модели:\\n\", similarity_scores_onnx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вероятность, что изображение соответствует тексту 1: 100.00%\n",
      "Вероятность, что изображение соответствует тексту 2: 0.01%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:5504: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# Загрузка модели и предобученных весов\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Подготовка изображения и текста\n",
    "image = preprocess(Image.open(\"image.jpg\")).unsqueeze(0).to(device)\n",
    "texts = clip.tokenize([\"a Guitar image\", \"A photo of a dog\"]).to(device)\n",
    "\n",
    "# Инференс\n",
    "with torch.no_grad():\n",
    "    # Получение эмбеддингов изображения и текста\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(texts)\n",
    "\n",
    "    # Нормализация эмбеддингов для более корректного вычисления сходства\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # Вычисление косинусного сходства\n",
    "    similarities = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "# Вывод результатов\n",
    "probs = similarities.squeeze().cpu().numpy()\n",
    "for i, prob in enumerate(probs):\n",
    "    print(f\"Вероятность, что изображение соответствует тексту {i + 1}: {prob:.2%}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
